{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916131ce",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 3 - Recommender Systems & Graph Mining (50 Points)\n",
    "\n",
    "Hello everyone, this assignment notebook covers the topics of Recommender Systems and Graph Mining. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e. your lines of code) between sentences that \"your code starts here\" and \"your code ends here\". The space between these two lines does not reflect the required or expected lines of code :). For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed)\n",
    "\n",
    "**Important:**\n",
    "* Remember to save this Jupyter notebook as A3_YourNameInLumiNUS_YourNUSNETID.ipynb\n",
    "* Submission deadline is Nov 7, 11.59 pm\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "Please add your nusnet and student id also in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab1ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0236597M'\n",
    "nusnet_id = 'e0744016'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f6f46",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Recommender Systems (30 Points)**\n",
    "    * 1.1 User-Based Collaborative Filtering (6 Points)\n",
    "        * 1.1 a) (4 Points)\n",
    "        * 1.2 b) (2 Points)\n",
    "    * 1.2 Implementing Matrix Factorization (14 Points)\n",
    "        * 1.2 a) (2 Points)\n",
    "        * 1.2 b) (2 Points)\n",
    "        * 1.2 c) (8 Points)\n",
    "        * 1.2 d) (2 Points)\n",
    "    * 1.3 Questions about Recommender Systems (10 Points)\n",
    "        * 1.3 a) (2 Points)\n",
    "        * 1.3 b) (2 Points)\n",
    "        * 1.3 c) (3 Points)\n",
    "        * 1.3 d) (3 Points)    \n",
    "* **2 Graph Mining - Centrality Measures (20 Points)**\n",
    "    * 2.1 Implementing PageRank (12 Points)\n",
    "        * 2.1 a) (2 Points)\n",
    "        * 2.1 b) (8 Points)\n",
    "        * 2.1 c) (2 Points)\n",
    "    * 2.2 Comparing Centrality Measures (8 Points)\n",
    "        * 2.2 a) (3 Points)\n",
    "        * 2.3 b) (5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07281c",
   "metadata": {},
   "source": [
    "### Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48d15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea1f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c7371",
   "metadata": {},
   "source": [
    "## 1 Recommender Systems (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b955e37",
   "metadata": {},
   "source": [
    "### 1.1 User-Based Collaborative Filtering (6 Points)\n",
    "\n",
    "Throughout this task, you are free to implement your calculations or using pen and paper, but please make sure that you provide sufficient details in your submitted answer that makes it clear how you got your results. The individual tasks specify this in more detail.\n",
    "\n",
    "Given to you is a simple rating dataset containing 6 users $u_1, u_2, \\dots, u_6$, and 8 movies $m_1, m_2, \\dots, m_8$, and the rating matrix $R$:\n",
    "\n",
    "$$\n",
    "R = \n",
    "\\begin{bmatrix} \n",
    "    5 & 0 & \\mathbf{\\color{red} ?} & 2 & 0 & 5 & 4 & 0 \\\\\n",
    "    3 & 0 & 4 & 0 & 0 & 2 & 0 & 3 \\\\\n",
    "    5 & 0 & 3 & 3 & 0 & 0 & 3 & 0 \\\\\n",
    "    5 & 0 & 0 & 3 & 0 & 4 & 5 & 0 \\\\\n",
    "    0 & 3 & 2 & 0 & 0 & 1 & 0 & 3 \\\\\n",
    "    5 & 3 & 1 & 3 & 0 & 5 & 4 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this example, the range of the ratings is from 1 to 5.\n",
    "\n",
    "Your task is to find the best estimate for rating $R_{u_1,m_3}$ of user $u_1$ for movie $m_3$, indicated by the red question mark in rating matrix $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b1fe4",
   "metadata": {},
   "source": [
    "**1.2 a) Calculate all cosine similarities between user $u_1$ and all other users! (4 Points)!** Please complete the list of equations in the markdown below; use a precision of 2 decimals.\n",
    "\n",
    "**Important:** Show at least for one equation how you calculate the similarity in detail. You can use an additional code or markdown cell.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(u1,u2): -0.29\n",
      "sim(u1,u3): 0.59\n",
      "sim(u1,u4): 0.74\n",
      "sim(u1,u5): -0.31\n",
      "sim(u1,u6): 0.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "M = np.array([[5,0,0,2,0,5,4,0],\n",
    "              [3,0,4,0,0,2,0,3],\n",
    "              [5,0,3,3,0,0,3,0],\n",
    "              [5,0,0,3,0,4,5,0],\n",
    "              [0,3,2,0,0,1,0,3],\n",
    "              [5,3,1,3,0,5,4,0]])\n",
    "\n",
    "masked = np.ma.masked_equal(M, 0)\n",
    "user_mean_ratings = np.mean(masked, axis=1).reshape(-1, 1)\n",
    "\n",
    "M_normalized = M - user_mean_ratings\n",
    "M_normalized[M == 0] = 0\n",
    "user_similarities = cosine_similarity(M_normalized, M_normalized)\n",
    "print('sim(u1,u2): {:.2f}'.format(user_similarities[0,1]))\n",
    "print('sim(u1,u3): {:.2f}'.format(user_similarities[0,2]))\n",
    "print('sim(u1,u4): {:.2f}'.format(user_similarities[0,3]))\n",
    "print('sim(u1,u5): {:.2f}'.format(user_similarities[0,4]))\n",
    "print('sim(u1,u6): {:.2f}'.format(user_similarities[0,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add930e7",
   "metadata": {},
   "source": [
    "* sim($u_1$, $u_2$) = <font color='red'>-0.29</font>\n",
    "* sim($u_1$, $u_3$) = <font color='red'>0.59</font>\n",
    "* sim($u_1$, $u_4$) = <font color='red'>0.74</font>\n",
    "* sim($u_1$, $u_5$) = <font color='red'>-0.31</font>\n",
    "* sim($u_1$, $u_6$) = <font color='red'>0.48</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef34d21",
   "metadata": {},
   "source": [
    "**1.2 b) Calculate the estimated rating $R_{u_1,m_3}$ (2 Points)!** Consider the 2 most similar users for this calculation; use a precision of 2 decimals. Show how you arrived at this result! You can use an additional code or markdown cell. \n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 1 rated movie 3 with: 2.1\n"
     ]
    }
   ],
   "source": [
    "user_idx = 0\n",
    "movie_idx = 2\n",
    "k = 2\n",
    "neighbors = np.argsort(user_similarities[user_idx])[::-1]\n",
    "neighbors = [ n_idx for i, n_idx in enumerate(neighbors) if M[n_idx][movie_idx] != 0 and user_similarities[user_idx][n_idx] > 0]\n",
    "topk_neighbor_indices = neighbors[:k]\n",
    "neighbor_similarities = user_similarities[user_idx][topk_neighbor_indices]\n",
    "neighbor_similarities\n",
    "neighbor_ratings = M[topk_neighbor_indices][:,movie_idx]\n",
    "if np.sum(neighbor_similarities) == 0:\n",
    "    user_rating = 0.0\n",
    "user_rating = np.round(np.sum(neighbor_similarities * neighbor_ratings) / np.sum(neighbor_similarities), 2)\n",
    "print('user 1 rated movie 3 with: {}'.format(user_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698bcf8",
   "metadata": {},
   "source": [
    "* $R_{u_1,m_3}$ = <font color='red'>2.1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2ec5e",
   "metadata": {},
   "source": [
    "### 1.2 Implementing Matrix Factorization\n",
    "\n",
    "Matrix Factorization -- and here more specifically: non-negative Matrix Factorization -- is a class of algorithms where a matrix $M$ is factorized into (usually) two matrices $W$ and $H$, with the property that all three matrices have no negative elements. Matrix Factorization is popular techniques applied in recommender systems, where $W$ and $H$ contain a latent representation of all users and all items, respectively, and $M$ represents the rating matrix.\n",
    "\n",
    "In this task, you will implement (non-negative) Matrix Factorization from scratch using Gradient Descent as covered in the lecture. In fact, we use the rating matrix $M$ which was used as an example in the lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b3af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 0. 0. 5. 1. 0. 0.]\n",
      " [5. 5. 4. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 4. 5. 0.]\n",
      " [0. 3. 0. 0. 0. 0. 3.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/miniconda3/envs/deeplearn_course/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "M = np.array([\n",
    "    [4, 0, 0, 5, 1, 0, 0],\n",
    "    [5, 5, 4, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 2, 4, 5, 0],\n",
    "    [0, 3, 0, 0, 0, 0, 3]\n",
    "], dtype=np.float)\n",
    "\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bbb23",
   "metadata": {},
   "source": [
    "**2.1 a) Implement method `mf_init()` that initializes matrices `W` and `H`, as well as matrix `Z`! (2 Points).** Matrix `Z` is a auxiliary matrix containing the indices of all non-zero entries of Matrix M (Hint: Have look at [`np.argwhere()`](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html) for this; below shows the expected shape and content of `Z`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6afcf149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_init(M, k=100):\n",
    "    # k is the size of the latent representation\n",
    "    \n",
    "    Z, W, H = None, None, None\n",
    "    \n",
    "    num_users, num_items = M.shape\n",
    "\n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################\n",
    "    W = np.random.rand(num_users, k)\n",
    "    H = np.random.rand(k, num_items)\n",
    "    Z = np.argwhere(M != 0)\n",
    "    \n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################  \n",
    "    \n",
    "    return Z, W, H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57511916",
   "metadata": {},
   "source": [
    "You can test your implementation with the code cell below. The shapes of `W` and `H` should reflect the number of users and items, as well as the size of the latent representations. The shape of `Z` should be `(num_nonzero, 2)`. For example matrix `M`, the shape should be `(11, 2)` since `M` has 11 non-zero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc604ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape = (4, 100)\n",
      "H.shape = (100, 7)\n",
      "Z.shape = (11, 2)\n",
      "\n",
      "Z containing all the indices of all non-zero entries in M\n",
      "[[0 0]\n",
      " [0 3]\n",
      " [0 4]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [2 3]\n",
      " [2 4]\n",
      " [2 5]\n",
      " [3 1]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "Z, W, H = mf_init(M)\n",
    "\n",
    "print('W.shape = {}'.format(W.shape))\n",
    "print('H.shape = {}'.format(H.shape))\n",
    "print('Z.shape = {}'.format(Z.shape))\n",
    "print()\n",
    "print('Z containing all the indices of all non-zero entries in M')\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065850c",
   "metadata": {},
   "source": [
    "The output for `Z` should look like this:\n",
    "```\n",
    "[[0 0]\n",
    " [0 3]\n",
    " [0 4]\n",
    " [1 0]\n",
    " [1 1]\n",
    " ...]\n",
    "```\n",
    "indicating the `M[0, 0]`, `M[0, 1]`, `M[0, 4]`, `M[1, 0]`, `M[1, 1]`, ... are all non-zero entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28ad0b",
   "metadata": {},
   "source": [
    "**1.2 b) Implement method `mf_loss()` to calculate the loss w.r.t. the current values of matrices `W` and `H`! (2 Points).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6295541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_loss(M, Z, W, H):\n",
    "    loss = np.inf\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################    \n",
    "    loss = np.sum(np.square(M-np.matmul(W,H))[M!=0])/len(Z)\n",
    "    \n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################    \n",
    "\n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81071a",
   "metadata": {},
   "source": [
    "You can test your implementation with the code cell below. We set `np.random.seed(0)` to ensure the same results in case of multiple executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8ccbea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 443.6\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "Z, W, H = mf_init(M)\n",
    "\n",
    "loss = mf_loss(M, Z, W, H)\n",
    "\n",
    "print('Initial loss: {:.1f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22efc9",
   "metadata": {},
   "source": [
    "The resulting loss should be: **443.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0dd51",
   "metadata": {},
   "source": [
    "**1.2 c) Complete the code cell below to implement matrix factorization using Gradient Descent! (8 Points).** The complete algorithm together with the required gradients is available as pseudo code in the lecture slides, and you are already familiar with the basic concept of Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4dc6f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 421.70195 \t 0%\n",
      "Loss: 264.70308 \t 10%\n",
      "Loss: 175.59000 \t 20%\n",
      "Loss: 120.98281 \t 30%\n",
      "Loss: 85.68139 \t 40%\n",
      "Loss: 61.95274 \t 50%\n",
      "Loss: 45.52764 \t 60%\n",
      "Loss: 33.89706 \t 70%\n",
      "Loss: 25.51235 \t 80%\n",
      "Loss: 19.37950 \t 90%\n",
      "Loss: 15.23625 \t 100%\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "k, learning_rate, lambda_reg, num_iter = 100, 0.0001, 0.1, 100\n",
    "\n",
    "# We first have to initialize\n",
    "np.random.seed(0)\n",
    "Z, W, H = mf_init(M, k=k)\n",
    "\n",
    "for it in range(num_iter):\n",
    "\n",
    "    #########################################################################################\n",
    "    ### Your code starts here ############################################################### \n",
    "    \n",
    "    # W = W + 2 * learning_rate*(np.matmul(M-np.matmul(W,H),H.transpose())+lambda_reg*W)\n",
    "    # H = H + 2 * learning_rate*(np.matmul(W.transpose(), M-np.matmul(W,H))+lambda_reg*H)\n",
    "    for i, j in Z:\n",
    "        u = W[i]\n",
    "        v = H[:,j]\n",
    "        grad_u = 2 * v * (M[i,j] - np.dot(u, v)) - 2 * lambda_reg * u\n",
    "        grad_v = 2 * u * (M[i,j] - np.dot(u, v)) - 2 * lambda_reg * v\n",
    "        W[i] += learning_rate * grad_u\n",
    "        H[:,j] += + learning_rate * grad_v\n",
    "\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################           \n",
    "\n",
    "    # Print loss every 10% of the iterations\n",
    "    if(it % (num_iter/10) == 0):\n",
    "        print('Loss: {:.5f} \\t {:.0f}%'.format(mf_loss(M, Z, W, H), (it / (num_iter/100))))\n",
    "        \n",
    "# Print final loss        \n",
    "print('Loss: {:.5f} \\t 100%'.format(mf_loss(M, Z, W, H)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443122fa",
   "metadata": {},
   "source": [
    "With the given hyperparameter values  (`k, learning_rate, lambda_reg, num_iter = 100, 0.0001, 0.1, 100`), you should see a loss around **15.2** at the end of the training.\n",
    "\n",
    "With our learned estimates for `W` and `H`, we can simply calculate matrix `P` as the product of `W` and `H`, representing the matrix of predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cef17269",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.dot(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f0232",
   "metadata": {},
   "source": [
    "Let's see how it looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e96227f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.  10.2 12.   7.9  5.6 10.6 12.5]\n",
      " [ 7.7  6.9  8.  11.2  9.1 14.9 13.1]\n",
      " [ 9.7  9.  10.4  7.   6.8  8.3 10.8]\n",
      " [ 9.1  7.2 10.7 11.7  9.1 12.4  9.3]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(P, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99053a48",
   "metadata": {},
   "source": [
    "With the given hyperparameter values (`k, learning_rate, lambda_reg, num_iter = 100, 0.0001, 0.1, 100`) the result should look something like this:\n",
    "\n",
    "```\n",
    "[[ 7.  10.2 12.   7.9  5.6 10.7 12.6]\n",
    " [ 7.8  6.9  8.1 11.3  9.1 15.  13.2]\n",
    " [ 9.7  9.  10.4  7.   6.8  8.4 10.8]\n",
    " [ 9.2  7.3 10.7 11.7  9.1 12.5  9.3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c3d31",
   "metadata": {},
   "source": [
    "**1.2 d) Explore different hyperparameter settings and briefly explain your observations! (2 Points)** You can simply re-run all code cells from 1.2 c) onwards, just with different parameter values for `k`, `learning_rate`, `lambda_reg`, and `num_iter`.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67227ff",
   "metadata": {},
   "source": [
    "1. k: k is the demension of the latent representation. With larger k, the system can caputure more information of the feature which means the larger the better, but usually 100 or 150 is enough.\n",
    "2. learning_rate: too large learning_rate may cause the loss to be `nan`.\n",
    "3. lambda_reg: increase the value of lambda_reg, we get worse fit of known ratings, \"smoother\" values for all ratings. decrease the value of lambda_reg, we get better fit of known ratings, more \"extreme\" values of unknown ratings.\n",
    "4. num_iter: large number of iteration may converge the loss to nearly 0, and the predition of known ratings are almost the same as the ground-truth ratings. The variance of the whole ratings are getting smaller with larger number of iterations. Which means the \"smoother\" values for all ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086f15d",
   "metadata": {},
   "source": [
    "### 1.3 Questions about Recommender Systems (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f7ac1",
   "metadata": {},
   "source": [
    "In 1.1, the movie $m_5$ is new has not received any rating yet, so $R_{u,m_5} = 0$ for any user $u$. Thus, using basic Collaborative Filtering, $m_5$ will never be recommended and therefore is less likely to get more ratings. \n",
    "\n",
    "**1.3 a) For real-world recommender systems, what are common strategies to mitigate this problem? (2 Points)** Briefly sketch some possible ideas.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0adea7",
   "metadata": {},
   "source": [
    "This is the cold-start problem for a new movie. We have no ratings of the new movie. Here are two ways to solve this problem:\n",
    "1. Use content-based model at first, we compute the similarity of new movies with other movies. When we get enough ratings of movies, we can change to collaborative filtering.\n",
    "2. An navie approach is to build a regression model for users with the independent variable of movie features, then use this model to predict ratings of new movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e50889",
   "metadata": {},
   "source": [
    "You get hired to build a recommendation system for a music streaming service. The platform already has 500,000 users and is giving them access to 1,000,000 songs. The platform recently introduced a feature where users can rate songs, but so far, only 10,000 ratings have been made.\n",
    "\n",
    "**1.3 b) In line with our classification used in the lecture, what type of recommender system is best suited for the current platform? (2 Points)** Briefly explain/justify your decision.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87767599",
   "metadata": {},
   "source": [
    "I would like to apply a hybrid content-based filtering recommender. There are only a few interactions, a pure collaborative algorithm can recommend it, but the quality of those recommendations will be poor, and it may not recommend unpopular songs at all. A hybrid approach can make the system robust because it considers the aspects of songs and users. In a hybrid system, characteristics, either of the songs or of the users, are weighted depending on the user’s sense of significance. Different criteria (such as the singers, languages, lyrics, and countries) will be weighted differently in order to select a song that the user would like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c06f7b",
   "metadata": {},
   "source": [
    "Assume you want to build movie recommendation platform that aggregates ratings of movies across different websites (e.g., [IMDb](https://www.imdb.com/), [Rotten Tomatoes](https://www.rottentomatoes.com/)). Basically all websites have their own way to quantify the movie ratings. Your goal is to build a recommender system using Collaborative Filtering.\n",
    "\n",
    "**1.3 c) Can you combine different rating datasets to make better recommendations, and if so, what are the requirements? (3 Points)**\n",
    "\n",
    "**Your answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014f599",
   "metadata": {},
   "source": [
    "Yes, they can be combined to make a better prediction as long as we make proper use of the data from the two websites.\n",
    "Both IMBd and Rottten Tomatoes have the same movies, but they have different evaluation systems for movies. RT is a 5-point system and imdb is a 10-point system. Therefore, the data must be unified first. While the two systems have different users, we can classify users and map the data on both sides according to user characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ac7b0",
   "metadata": {},
   "source": [
    "In 1.2 you've implemented a model-based recommender system using (non-negative) Matrix Factorization. Since we used only a toy rating matrix, performance was not an issue here. In real-world recommendations with many users and items, Matrix Factorization can be quite time consuming. The problem is that online platforms are very dynamic: users are joining and leaving, new items are added, users add new or update previous ratings. All of those cases change the rating matrix.\n",
    "\n",
    "**1.3 d) How do different cases (e.g., new user/item/rating) affect a current result of a Matrix Factorization for a recommender system? (3 Points)** Outline the different problems, and discuss meaningful approaches to mitigate them. For example, a new user or item refers to the *Cold-Start Problem*. What are good strategies to address the Cold-Start Problem using Matrix Factorization?\n",
    "\n",
    "(Note: When you're discussing challenges regarding runtime/performance, please **exclude** any solutions relying on bigger clusters and parallel computing :). While those are valid points, in principle, here we want to focus on conceptual solutions).\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e7b44",
   "metadata": {},
   "source": [
    "1. User cold start problem: A basic MF model can’t predict ratings for new users. Use Hybrid MF-Decision Tree Recommender System to solve the problem. Uses MF to find best queries to estimate user profiles.\n",
    "2. Item cold start problem: How to recommend new unrated items in MF framework? Use Local Collective Embeddings to solve the problem. Uses collective factorization to link user ratings to item text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef4d06",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d1ed6",
   "metadata": {},
   "source": [
    "## 2 Graph Mining\n",
    "\n",
    "For your last task, we look into the concept of centrality measures which quantify the importance of nodes in a graph/network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9171b6",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "As graph dataset, we use the MRT network in Singapore collected from the [LTA DataMall](https://datamall.lta.gov.sg/content/dam/datamall/datasets/Geospatial/TrainStation.zip); note that we exclude LRT stations. The data comes in goe-spatial data format with several additional metadata. However, we already prepared the data and converted it into simple CSV files.\n",
    "\n",
    "In the following, we load and prepare the data for the task. You can just run through the following code cells to get everything set up.\n",
    "\n",
    "\n",
    "#### Load MRT Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6045de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>codes</th>\n",
       "      <th>stn_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Admiralty</td>\n",
       "      <td>NS10</td>\n",
       "      <td>ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aljunied</td>\n",
       "      <td>EW9</td>\n",
       "      <td>if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>NS16</td>\n",
       "      <td>vs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bakau</td>\n",
       "      <td>SE3</td>\n",
       "      <td>cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bangkit</td>\n",
       "      <td>BP9</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name codes stn_id\n",
       "0   Admiralty  NS10     ba\n",
       "1    Aljunied   EW9     if\n",
       "2  Ang Mo Kio  NS16     vs\n",
       "3       Bakau   SE3     cl\n",
       "4     Bangkit   BP9     id"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mrt_stations = pd.read_csv('data/a3-sg-mrt-stations.csv')\n",
    "\n",
    "df_mrt_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784dbc0",
   "metadata": {},
   "source": [
    "#### Load MRT Connections\n",
    "\n",
    "The data of the train connections contains information about each direct connection between 2 MRT stations. Each MRT stations is identified by its unique name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe2ff110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>novena</td>\n",
       "      <td>newton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pasir ris</td>\n",
       "      <td>tampines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sengkang</td>\n",
       "      <td>punggol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>woodlands</td>\n",
       "      <td>marsiling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gul circle</td>\n",
       "      <td>tuas crescent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source    destination\n",
       "0      novena         newton\n",
       "1   pasir ris       tampines\n",
       "2    sengkang        punggol\n",
       "3   woodlands      marsiling\n",
       "4  gul circle  tuas crescent"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mrt_connections = pd.read_csv('data/a3-sg-mrt-connections.csv')\n",
    "\n",
    "df_mrt_connections.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689af14",
   "metadata": {},
   "source": [
    "#### Generate Graph\n",
    "\n",
    "From the connection data, we can now generate the train network graph where the nodes are MRT stations and there is a directed edge between two nodes if there is a direct connection between the corresponding MRT stations. While the travel time between two MRT stations would make a meaningful edge weight, we ignore it in this notebook for simplicity and assume an unweighted graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e4e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an \"empty\" directed graph G\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for idx, row in df_mrt_connections.iterrows():\n",
    "    G.add_edge(row['source'], row['destination'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee8b919",
   "metadata": {},
   "source": [
    "#### Basic Information about the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d9fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph has 157 nodes and 354 edges\n"
     ]
    }
   ],
   "source": [
    "print('The graph has {} nodes and {} edges'.format(len(G.nodes), len(G.edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12f2d1",
   "metadata": {},
   "source": [
    "#### Create Adjacency Matrix for Graph G\n",
    "\n",
    "We later also need the adjacency matrix of the MRT network graph. We can simple use [`to_numpy_matrix()`](https://networkx.org/documentation/networkx-1.7/reference/generated/networkx.convert.to_numpy_matrix.html) which returns the graph adjacency matrix as a numpy matrix. As the identifier of nodes are the names of the MRT stations but the matrix can only be accessed via its row and column indices, we also need a dictionary that maps the indices back to the station names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14d93392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novena\n"
     ]
    }
   ],
   "source": [
    "## Convert graph of train connection to adjacency matrix\n",
    "A_mrt = nx.to_numpy_matrix(G)\n",
    "\n",
    "## Create a dictionary to map a matrix index to the MRT station name (which are the node identifiers)\n",
    "node_map = { idx:node for idx, node in enumerate(G.nodes)  }\n",
    "\n",
    "## Print an example\n",
    "print(node_map[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903f541",
   "metadata": {},
   "source": [
    "### 2.1 Implementing PageRank (14 Points)\n",
    "\n",
    "In this task, you will implement the basic PageRank algorithm using the Power Iteration methods as introduced in the lecture.\n",
    "\n",
    "$$\n",
    "c_{PR} = \\alpha M c_{PR} + (1-\\alpha)E\n",
    "$$\n",
    "\n",
    "where $E = (1/n, 1/n, ..., 1/n)^T$ with $n$ being the number of nodes.\n",
    "\n",
    "For testing and debugging your implementation, define a very basic graph by means of its adjacency matrix. In fact, this toy graph is the one used in the [original PageRank paper](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf) (Page 5) from 1999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99cf27f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/miniconda3/envs/deeplearn_course/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "A_demo = np.array([\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0]    \n",
    "], dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49315799",
   "metadata": {},
   "source": [
    "**2.1 a) Implement method `create_transition_matrix()` that converts an adjacency matrix to a transition matrix! (2 Points).** (Hint: You can find an example for this transformation in the lecture slides.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21680c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 1. ],\n",
       "       [0.5, 0. , 0. ],\n",
       "       [0.5, 1. , 0. ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_transition_matrix(A):\n",
    "    \n",
    "    M = None\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################    \n",
    "    M = A.T\n",
    "\n",
    "    M = M / M.sum(axis=0)\n",
    "\n",
    "    M = np.nan_to_num(M)\n",
    "\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################\n",
    "    \n",
    "    return np.asarray(M)\n",
    "\n",
    "\n",
    "create_transition_matrix(A_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b0251",
   "metadata": {},
   "source": [
    "**2.1 b) Implement method `pagerank()` to compute the PageRank scores given an adjacency matrix A (8 Points).** (Hint: You can find an example for this transformation in the lecture slides.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4e882ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(A, alpha=0.85, eps=0.0001, max_iter=1000, verbose=True):\n",
    "    \n",
    "    ## Generate transition matrix from adjacency matrix A\n",
    "    M = create_transition_matrix(A)\n",
    "    E, c = None, None\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ############################################################### \n",
    "\n",
    "    ## Initialize E and c\n",
    "    num_nodes = A.shape[0]\n",
    "    E = np.ones(num_nodes)/num_nodes\n",
    "    c = alpha * np.matmul(M, E) + (1-alpha) * E\n",
    "    \n",
    "    ### Your code ends here #################################################################\n",
    "    ######################################################################################### \n",
    "\n",
    "    # Run the power method: iterate until differences between steps converges\n",
    "    num_iter = 0\n",
    "    while True:\n",
    "        \n",
    "        num_iter += 1\n",
    "\n",
    "        #########################################################################################\n",
    "        ### Your code starts here ###############################################################  \n",
    "        old_c = c\n",
    "        c = alpha * np.matmul(M, c) + (1-alpha) * E\n",
    "\n",
    "        if (np.abs(c-old_c) <= eps).all():\n",
    "            break\n",
    "            \n",
    "        ### Your code ends here #################################################################\n",
    "        #########################################################################################            \n",
    "            \n",
    "        \n",
    "    if verbose == True:\n",
    "        print('The power method took {} iterations.'.format(num_iter))\n",
    "\n",
    "    ## Return the results as a dictiory with the nodes as keys and the PageRank score as values\n",
    "    return { k:score for k, score in enumerate(c.squeeze()) }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf74b9",
   "metadata": {},
   "source": [
    "Test your implementation on the toy graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cbb053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power method took 23 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.3999837239583333, 1: 0.20003255208333331, 2: 0.3999837239583333}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagerank(A_demo, alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214e70e",
   "metadata": {},
   "source": [
    "With $\\alpha=1.0$ the scores should look as follows -- apart from floating point precision issues:\n",
    "\n",
    "`{0: 0.4, 1: 0.2, 2: 0.4}`\n",
    "\n",
    "Again, you can check with the [original PageRank paper](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf) (Page 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1443b",
   "metadata": {},
   "source": [
    "**2.1 c) Run your PageRank implementation on the MRT train network graph (2 Points)**. Find the top-5 MRT stations with respect to their PageRank scores and complete the table below. Use the code cell below to show how you get to the results; use the default value $\\alpha = 0.85$. (Hint: You will need `node_map` name get the name of the MRT stations).\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "\n",
    "| Rank | Score | MRT Station |\n",
    "| ---  | ---   | ---                  |\n",
    "| 1    | <font color='red'>0.01499</font> | <font color='red'>sengkang</font> |\n",
    "| 2    | <font color='red'>0.01315</font> | <font color='red'>punggol</font> |\n",
    "| 3    | <font color='red'>0.01126</font> | <font color='red'>dhoby ghaut</font> |\n",
    "| 4    | <font color='red'>0.01116</font> | <font color='red'>tampines</font> |\n",
    "| 5    | <font color='red'>0.01083</font> | <font color='red'>bukit panjang</font>  |\n",
    "\n",
    "(for the scores, please use a precision of 5 decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e38dedeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power method took 17 iterations.\n",
      "top 5 score: [0.01499, 0.01315, 0.01126, 0.01116, 0.01083]\n",
      "top 5 name: ['sengkang', 'punggol', 'dhoby ghaut', 'tampines', 'bukit panjang']\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "mrt_scores = pagerank(A_mrt, alpha=0.85)\n",
    "top_5 = sorted(mrt_scores.items(),key = lambda x:x[1],reverse = True)[0:5]\n",
    "top_5_score = [round(x[1],5) for x in top_5]\n",
    "top_5_id = [x[0] for x in top_5]\n",
    "top_5_name = [node_map[id] for id in top_5_id]\n",
    "print('top 5 score:', top_5_score)\n",
    "print('top 5 name:', top_5_name)\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38b47f",
   "metadata": {},
   "source": [
    "### 2.2 Comparing Centrality Measures (8 Points)\n",
    "\n",
    "We saw in the lecture that different centrality measures look at different topological features of a graph to quantify the importance of nodes. This task compares different measures, using the following implementations provided by `networkX`:\n",
    "\n",
    "* [nx.algorithms.link_analysis.pagerank](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html)\n",
    "* [nx.centrality.in_degree_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.in_degree_centrality.html#networkx.algorithms.centrality.in_degree_centrality)\n",
    "* [nx.centrality.out_degree_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.out_degree_centrality.html#networkx.algorithms.centrality.out_degree_centrality)\n",
    "* [nx.centrality.closeness_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.closeness_centrality.html#networkx.algorithms.centrality.closeness_centrality)\n",
    "* [nx.centrality.betweenness_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html)\n",
    "\n",
    "(**Note:** [nx.algorithms.link_analysis.pagerank](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html) might give a (slightly) different ranking than your own implementation of PageRank since this implementation is a bit modified. So don't take this as a 1:1 reference to check your PageRank implementation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78432899",
   "metadata": {},
   "source": [
    "**2.2 a) Run the 5 centrality measures on the MRT train network graph (3 Points)**. Find the top-5 MRT stations with respect to their centrality scores and complete the table below. You only need to add the name of the MRT stations, not the scores.\n",
    "\n",
    "Use the code cell below to show how you get to the results; use the default values for all 5 implementations of the centrality measures.\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "\n",
    "| Rank | PageRank | InDegree | OutDegree | Closeness | Betweenness |\n",
    "| ---  | ---      | ---      | ---       | ---         |  --- |\n",
    "| 1    |  <font color='red'>sengkang</font> | <font color='red'>sengkang</font>  | <font color='red'>sengkang</font>  | <font color='red'>serangoon</font> |  <font color='red'>serangoon</font>  |\n",
    "| 2    |  <font color='red'>punggol</font>  | <font color='red'>punggol</font>  | <font color='red'>punggol</font> | <font color='red'>bishan</font>  | <font color='red'>botanic gardens</font>  |\n",
    "| 3    | <font color='red'>dhoby ghaut</font>  | <font color='red'>dhoby ghaut</font>   | <font color='red'>dhoby ghaut</font> | <font color='red'>botanic gardens</font>  | <font color='red'>kovan</font>  |\n",
    "| 4    | <font color='red'>tampines</font>  | <font color='red'>newton</font> |  <font color='red'>newton</font> |  <font color='red'>little india</font> | <font color='red'>bishan</font>  |\n",
    "| 5    | <font color='red'>bukit panjang</font>  | <font color='red'>tampines</font> | <font color='red'>tampines</font>  |  <font color='red'>lorong chuan</font> | <font color='red'>sengkang</font>  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbe83c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "page_rank = sorted(nx.pagerank(G, alpha=0.85).items(),key = lambda x:x[1],reverse = True)[0:5]\n",
    "in_degree = sorted(nx.in_degree_centrality(G).items(),key = lambda x:x[1],reverse = True)[0:5]\n",
    "out_degree = sorted(nx.out_degree_centrality(G).items(),key = lambda x:x[1],reverse = True)[0:5]\n",
    "closeness = sorted(nx.closeness_centrality(G).items(),key = lambda x:x[1],reverse = True)[0:5]\n",
    "betweenness = sorted(nx.betweenness_centrality(G).items(),key = lambda x:x[1],reverse = True)[0:5]\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bd3e0",
   "metadata": {},
   "source": [
    "**2.2 b) Discuss the results and your observations (5 Points).** Based on the definitions and intuitions behind these 5 different centrality measures, discuss the results of 2.2 a): For each centrality, briefly describe what it means for a MRT station to have the highest score!\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c531aa",
   "metadata": {},
   "source": [
    "1. PageRank: PageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links. In the case of MRT, it returns the station which intersect most with other stations. There are three lines coming into Sengkang MRT.\n",
    "2. InDegree: The in-degree centrality for a node v is the fraction of nodes its incoming edges are connected to. In the case of MRT, it returns the station which intersect most with other stations. \n",
    "3. OutDegree: The out-degree centrality for a node v is the fraction of nodes its outgoing edges are connected to. In the case of MRT, it is similar to the results of InDegree algorithm.\n",
    "4. Closeness: Closeness centrality of a node u is the reciprocal of the sum of the shortest path distances from u to all n-1 other nodes. In the case of MRT, it returns the station which is located in the center of Singapore. Since the top five MRT stations are all on the circle line or in the circle line.\n",
    "5. Betweenness: Betweenness centrality of a node v is the sum of the fraction of all-pairs shortest paths that pass through v. In the case of MRT, it is similar to the Closeness algorithm. The MRT in the center of Singapore tend to have higher score.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fdee1e21c7b5d36f03fc9ea81859adc2adf10eecfa08bafeab8ab5a4d8bd510"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('deeplearn_course': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
