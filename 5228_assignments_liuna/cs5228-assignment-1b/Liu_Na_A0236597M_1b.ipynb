{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS5228 Assignment 1b - EDA, Data Cleaning, Association Rules (50 Points)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hello everyone, this assignment notebook covers the topic of Exploratory Data Analysis, Data Cleaning, & Association Rules. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e. your lines of code) between sentences that \"your code starts here\" and \"your code end here\". The space between these two lines does not reflect reflect the required or expected lines of code :). For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed) \n",
    "\n",
    "**Important:** \n",
    "* Remember to save this Jupyter notebook as A1a_YourNameInLumiNUS_YourNUSNETID.ipynb\n",
    "* Please upload only this notebook directly to LumiNUS (no other files, not as zipped archive)\n",
    "* Submission deadline is September 19th, 11.59 pm (together with A1a)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Please also add your nusnet and student id also in the code cell below. This is just to make any identification of your notebook doubly sure."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "student_id = 'A0236597M'\n",
    "nusnet_id = 'e0744016'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts are provide additional explanations, documentation, or some discussion. The code and markdown cells you are a supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "\n",
    "* **1. Exploratory Data Analysis (EDA) & Data Cleaning (25 Points)**\n",
    "    * 1.1 Cleaning a Real-World Dataset (12 Points)\n",
    "    * 1.2 Basic Facts about the Dataset (8 Points)\n",
    "    * 1.3 Discussion (5 Points)\n",
    "* **2 Association Rule Mining - Apriori Algorithm (25 Points)**\n",
    "    * 2.1 Generate & Prune (k+1)-itemsets (5 Points)\n",
    "    * 2.2 Generate Frequent Itemsets with Apriori Algorithm (5 Points)\n",
    "    * 2.3 Association Rule Mining over Real-World Dataset (COVID-19) (9 Points)\n",
    "        * 2.3a) (3 Points)\n",
    "        * 2.3b) (3 Points)\n",
    "        * 2.3c) (3 Points)\n",
    "    * 2.4 Comparison with Different Use Case (Market Basket Analysis) (6 Points)\n",
    "        * 2.4a) (4 Points)\n",
    "        * 2.4b) (2 Points)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the Notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "# This will automatically reload src/dtree.py every time you make changes and save the file\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following statements contain all the packages need to complete the notebook. Note that this notebook will provide you with a series of methods to help you solving the programming tasks. The purpose and the output of each auxiliary method is pretty straightforward, but we will provide examples throughout the notebook.\n",
    "\n",
    "**Important:** For this notebook, you need to install the [`efficient-apriori`](https://pypi.org/project/efficient-apriori/) Python package. This is required for the task covering Association Rules."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from efficient_apriori import apriori\n",
    "from src.utils import unique_items, powerset, support, confidence, merge_itemsets, generate_association_rules, show_top_rules"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Exploratory Data Analysis (EDA) & Data Cleaning (25 Points)\n",
    "\n",
    "### 1.1 Cleaning a Real-World Dataset (12 Points)\n",
    "\n",
    "Assume that you have been tasked to build a regression model to predict the resale prices of HDB flats in Singapore. To this end you get a dataset containing information about 20,000 past resale transaction, including a documentation with the following information about the attributes:\n",
    "\n",
    "* **transaction_id**: Unique ID of the transactions. A 6-digit integer number uniquely assigned to each transaction. If this code starts with the letter 'C', it indicates a cancellation of the transaction.\n",
    "* **town**: The town in which the flat is located, e.g., \"bukit merah\", \"woodlands\".\n",
    "* **flat_type**: Type of flat, e.g., \"3 room\", \"4 room\", \"executive\".\n",
    "* **street_name**: Name of street.\n",
    "* **storey_range**: Description on what storey the flat is located.\n",
    "* **floor_area_sqm**: Living area of flat in square meter.\n",
    "* **flat_model**: Model of the flat, e.g., \"model a\", \"improved\".\n",
    "* **lease_commence_date**: Year when the lease of the flat commenced, e.g., 1999, 1984\n",
    "* **resale_price**: Resale price of the flat in Singapore dollar.\n",
    "\n",
    "Let's have a first look at the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_hdb = pd.read_csv('data/a1-resale-flat-prices.csv')\n",
    "\n",
    "df_hdb.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  transaction_id           town flat_type block     street_name storey_range  \\\n",
       "0         545066       sengkang    4 room  201C  compassvale dr     07 to 09   \n",
       "1         539856        punggol    3 room  664A      punggol dr     04 to 06   \n",
       "2         535785  bukit panjang    3 room   251      bangkit rd     07 to 09   \n",
       "3         537535        punggol    4 room  213C    punggol walk     04 to 06   \n",
       "4         531556         yishun    3 room   629    yishun st 61     10 to 12   \n",
       "\n",
       "   floor_area_sqm flat_model  lease_commence_date  resale_price  \n",
       "0            90.0    model a                 2001      378000.0  \n",
       "1            65.0    model a                 2016      342888.0  \n",
       "2            73.0    model a                 1988      252000.0  \n",
       "3            93.0    model a                 2015      435000.0  \n",
       "4            73.0    model a                 1988      280000.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>town</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>block</th>\n",
       "      <th>street_name</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>flat_model</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545066</td>\n",
       "      <td>sengkang</td>\n",
       "      <td>4 room</td>\n",
       "      <td>201C</td>\n",
       "      <td>compassvale dr</td>\n",
       "      <td>07 to 09</td>\n",
       "      <td>90.0</td>\n",
       "      <td>model a</td>\n",
       "      <td>2001</td>\n",
       "      <td>378000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>539856</td>\n",
       "      <td>punggol</td>\n",
       "      <td>3 room</td>\n",
       "      <td>664A</td>\n",
       "      <td>punggol dr</td>\n",
       "      <td>04 to 06</td>\n",
       "      <td>65.0</td>\n",
       "      <td>model a</td>\n",
       "      <td>2016</td>\n",
       "      <td>342888.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>535785</td>\n",
       "      <td>bukit panjang</td>\n",
       "      <td>3 room</td>\n",
       "      <td>251</td>\n",
       "      <td>bangkit rd</td>\n",
       "      <td>07 to 09</td>\n",
       "      <td>73.0</td>\n",
       "      <td>model a</td>\n",
       "      <td>1988</td>\n",
       "      <td>252000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>537535</td>\n",
       "      <td>punggol</td>\n",
       "      <td>4 room</td>\n",
       "      <td>213C</td>\n",
       "      <td>punggol walk</td>\n",
       "      <td>04 to 06</td>\n",
       "      <td>93.0</td>\n",
       "      <td>model a</td>\n",
       "      <td>2015</td>\n",
       "      <td>435000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>531556</td>\n",
       "      <td>yishun</td>\n",
       "      <td>3 room</td>\n",
       "      <td>629</td>\n",
       "      <td>yishun st 61</td>\n",
       "      <td>10 to 12</td>\n",
       "      <td>73.0</td>\n",
       "      <td>model a</td>\n",
       "      <td>1988</td>\n",
       "      <td>280000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Perform EDA on the HDB Resale Price dataset and perform appropriate preprocessing steps to clean the data!**\n",
    "The preprocessing step for cleaning the data step may include \n",
    "* the removal of \"dirty records\" (i.e., records that do not adhere to the data description) and\n",
    "* the modification of records\n",
    "\n",
    "In the following, **identify at least 5 issues** with the dataset that would negatively affect any subsequent analysis, and clean the data accordingly.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "* Recall from the lecture that data cleaning often involves to make certain decisions. As such, you might come up with different steps than other students. This is OK as long as you can reasonably justify your steps.\n",
    "* Perform the data cleaning on a copy of the original dataset `df_hdb_cleaned`; see code cell below. Later tasks will work on the original dataset `df_hdb` to ensure that the result are consistent and do not depend on your choice of data preprocessing.\n",
    "* The goals is to preserve as much of the records as possible! So only remove records as part of your data cleaning if it's really necessary (this includes that you should not remove any attributes!). There might be different valid cases, so don't forget to briefly justify yout decision.\n",
    "\n",
    "Please provide your answer below. It should list the different issues you have identified and briefly discuss which data cleaning steps you can and/or need to perform to address those issues.\n",
    "\n",
    "**Your answer:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. remove the rows which include `NaN`\n",
    "2. remove the column `transaction_id`\n",
    "3. change `storey_range` into arrays: 07 to 09 ➡️ [7,8,9]; 04 to 06 ➡️ [4,5,6]\n",
    "4. remove the rows where `lease_commence_date` is greater than 2021\n",
    "5. rescale the data of `resale_price`: `resale_price = resale_price/10000`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the code cell below to actually implement your steps for cleaning clean the data. The results should back up your answer above. Feel free to split the cell into multiple code cells to improve organization (not a must, though).\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the codes you have to complete -- `pandas` is your best friend here :). If you use loops but the results is correct, there will be some minor deduction of points."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_hdb_cleaned = df_hdb.copy()\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "# 1. remove NaN\n",
    "df_hdb_cleaned =  df_hdb_cleaned.dropna()\n",
    "\n",
    "# 2. remove the column transaction_id\n",
    "df_hdb_cleaned = df_hdb_cleaned.drop(['transaction_id'], axis=1)\n",
    "\n",
    "# 3. change storey_range into arrays\n",
    "def update_storey_range(str):\n",
    "    a = [int(i.strip()) for i in str.split('to')]\n",
    "    b = np.arange(a[0], a[1]+1)\n",
    "    return b\n",
    "\n",
    "df_hdb_cleaned['storey_range'] = df_hdb_cleaned['storey_range'].apply(update_storey_range)\n",
    "\n",
    "# 4. remove the rows where lease_commence_date is greater than 2021\n",
    "df_hdb_cleaned = df_hdb_cleaned[df_hdb_cleaned['lease_commence_date']<2021]\n",
    "\n",
    "# 5. rescale the sale_price\n",
    "df_hdb_cleaned['resale_price'] = (df_hdb_cleaned['resale_price']/10000).round(2)\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "print('After preprocessing, There are now {} entries.'.format(df_hdb_cleaned.shape[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After preprocessing, There are now 16724 entries.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Basic Facts about a Real-World Dataset (8 Points)\n",
    "\n",
    "This task is about getting basic insights into a given dataset. For this, we use the \"Online Retail II\" dataset. It contains all the transactions occurring for a UK-based and registered, non-store online retail. The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers (Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00502/)\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "* **Invoice**: Invoice number. A 6-digit integral number uniquely assigned to each transaction. \n",
    "* **StockCode**: Product (item) code. A 5-digit integral number uniquely assigned to each distinct product. \n",
    "* **Description**: Product (item) name. A string with letters being UPPERCASE.\n",
    "* **Quantity**: The quantities of each product (item) per transaction. A simple integer value.\n",
    "* **InvoiceDate**: Invoice date and time. A string representing the day and time when a transaction was generated.\n",
    "* **Price**: Unit price. A numeric value representing the product price per unit in sterling.\n",
    "* **CustomerID**: Customer number. A 5-digit integral number uniquely assigned to each customer.\n",
    "\n",
    "**Important:** This is not the raw dataset from the source linked above, but has already been cleaned! Also, each (Invoice,StockCode) value pair is unique -- that is, each transaction contains the same StockCode at most once."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df_retail = pd.read_csv('data/a1-online-retail-cleaned.csv')\n",
    "\n",
    "df_retail.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Invoice  StockCode                        Description  Quantity  \\\n",
       "0   489435      22350                           CAT BOWL        12   \n",
       "1   489435      22349     DOG BOWL , CHASING BALL DESIGN        12   \n",
       "2   489435      22195       HEART MEASURING SPOONS LARGE        24   \n",
       "3   489435      22353  LUNCHBOX WITH CUTLERY FAIRY CAKES        12   \n",
       "4   489440      22350                           CAT BOWL         8   \n",
       "5   489440      22349     DOG BOWL , CHASING BALL DESIGN         8   \n",
       "6   489448      20827                GOLD APERITIF GLASS        48   \n",
       "7   489448      20825                    GOLD WINE GLASS        48   \n",
       "8   489448      20823                   GOLD WINE GOBLET        48   \n",
       "9   489448      20826              SILVER APERITIF GLASS        48   \n",
       "\n",
       "         InvoiceDate  Price  CustomerID  \n",
       "0  12/01/09 07:46 AM   2.55       13085  \n",
       "1  12/01/09 07:46 AM   3.75       13085  \n",
       "2  12/01/09 07:46 AM   1.65       13085  \n",
       "3  12/01/09 07:46 AM   2.55       13085  \n",
       "4  12/01/09 09:43 AM   2.55       18087  \n",
       "5  12/01/09 09:43 AM   3.75       18087  \n",
       "6  12/01/09 10:18 AM   2.12       15413  \n",
       "7  12/01/09 10:18 AM   3.39       15413  \n",
       "8  12/01/09 10:18 AM   4.25       15413  \n",
       "9  12/01/09 10:18 AM   2.12       15413  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489435</td>\n",
       "      <td>22350</td>\n",
       "      <td>CAT BOWL</td>\n",
       "      <td>12</td>\n",
       "      <td>12/01/09 07:46 AM</td>\n",
       "      <td>2.55</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489435</td>\n",
       "      <td>22349</td>\n",
       "      <td>DOG BOWL , CHASING BALL DESIGN</td>\n",
       "      <td>12</td>\n",
       "      <td>12/01/09 07:46 AM</td>\n",
       "      <td>3.75</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489435</td>\n",
       "      <td>22195</td>\n",
       "      <td>HEART MEASURING SPOONS LARGE</td>\n",
       "      <td>24</td>\n",
       "      <td>12/01/09 07:46 AM</td>\n",
       "      <td>1.65</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489435</td>\n",
       "      <td>22353</td>\n",
       "      <td>LUNCHBOX WITH CUTLERY FAIRY CAKES</td>\n",
       "      <td>12</td>\n",
       "      <td>12/01/09 07:46 AM</td>\n",
       "      <td>2.55</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489440</td>\n",
       "      <td>22350</td>\n",
       "      <td>CAT BOWL</td>\n",
       "      <td>8</td>\n",
       "      <td>12/01/09 09:43 AM</td>\n",
       "      <td>2.55</td>\n",
       "      <td>18087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>489440</td>\n",
       "      <td>22349</td>\n",
       "      <td>DOG BOWL , CHASING BALL DESIGN</td>\n",
       "      <td>8</td>\n",
       "      <td>12/01/09 09:43 AM</td>\n",
       "      <td>3.75</td>\n",
       "      <td>18087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>489448</td>\n",
       "      <td>20827</td>\n",
       "      <td>GOLD APERITIF GLASS</td>\n",
       "      <td>48</td>\n",
       "      <td>12/01/09 10:18 AM</td>\n",
       "      <td>2.12</td>\n",
       "      <td>15413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>489448</td>\n",
       "      <td>20825</td>\n",
       "      <td>GOLD WINE GLASS</td>\n",
       "      <td>48</td>\n",
       "      <td>12/01/09 10:18 AM</td>\n",
       "      <td>3.39</td>\n",
       "      <td>15413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>489448</td>\n",
       "      <td>20823</td>\n",
       "      <td>GOLD WINE GOBLET</td>\n",
       "      <td>48</td>\n",
       "      <td>12/01/09 10:18 AM</td>\n",
       "      <td>4.25</td>\n",
       "      <td>15413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>489448</td>\n",
       "      <td>20826</td>\n",
       "      <td>SILVER APERITIF GLASS</td>\n",
       "      <td>48</td>\n",
       "      <td>12/01/09 10:18 AM</td>\n",
       "      <td>2.12</td>\n",
       "      <td>15413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Complete the table below by answering the 8 given questions!** Use the code cell below the table to actually implement your steps that enabled you to answer the questions. Read all questions carefully to make sure you provide the correct answer (Hint: If the 10 example entries shown above are not sufficient, you can always open `data/a1-online-retail-cleaned.csv` in Excel, text editor, etc. if you want to have a closer look at the data)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(8).\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| 1)  | What are the dates of the **first transaction** and the **last transaction**?                                                                              | 12/01/09 07:46 AM, 12/09/10 07:28 PM |\n",
    "| 2)  | What are the **unique number of items** and **the unique number of customers**?                                                                                | 2263, 2181 |\n",
    "| 3)  | How many transactions included at **least 10** items with **StockCode 84839**?                                                                                       | 8 |\n",
    "| 4)  | Which transaction contains the **highest number of individual items** (give the Invoice number and the number of individual items)?                                                                        | 502269, 1000 |\n",
    "| 5)  | What was the **most expensive transaction** (give the Invoice number and the total price)?                                                                                    | 530715, 15818.4 |\n",
    "| 6)  | How many transactions sold **at least one CAKESTAND**?                                                         | 447 |\n",
    "| 7)  | Which customer has made the **most transactions** and how many (give the Customer ID and the number of transactions)?                                                        | 14646, 422 |\n",
    "| 8)  | Which item has **sold the most** across all transactions (given the StockCode and the number of sold items)? |22423, 407 |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important:** Avoid using loops in the parts of the codes you have to complete; again, check out the in-built methods that `pandas` provides. If you use loops but the results is correct, there will be some minor deduction of points."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "# 1)\n",
    "first_transaction_date = df_retail['InvoiceDate'][0]\n",
    "last_transaction_date = df_retail['InvoiceDate'][47142]\n",
    "print('first_transaction_date:', first_transaction_date)\n",
    "print('last_transaction_date:', last_transaction_date)\n",
    "# 2)\n",
    "unique_num_items = len(df_retail['StockCode'].unique())\n",
    "unique_num_customers = len(df_retail['CustomerID'].unique())\n",
    "print('unique_num_items:', unique_num_items)\n",
    "print('unique_num_customers:', unique_num_customers)\n",
    "# 3)\n",
    "transactions_with_84839 = len(df_retail[(df_retail['StockCode']==84839) & (df_retail['Quantity']>=10)])\n",
    "print('transactions_with_84839:', transactions_with_84839)\n",
    "# 4)\n",
    "max_quantity = df_retail['Quantity'].max()\n",
    "Invoice = df_retail[df_retail['Quantity']==max_quantity]['Invoice']\n",
    "print('max_quantity:', max_quantity)\n",
    "print('Invoice:', Invoice)\n",
    "# 5)\n",
    "df_retail['Cost'] = df_retail['Price']*df_retail['Quantity']\n",
    "total_price = df_retail['Cost'].max()\n",
    "Invoice = df_retail[df_retail['Cost']==total_price]['Invoice']\n",
    "print('Invoice:', Invoice)\n",
    "print('total_price:', total_price)\n",
    "# 6)\n",
    "number_cakestand = len(df_retail[df_retail['Description'].str.contains('CAKESTAND')])\n",
    "print('number_cakestand:', number_cakestand)\n",
    "# 7)\n",
    "df_retail['CustomerID'] = df_retail['CustomerID'].astype(str)\n",
    "most_freq_customer_id = df_retail['CustomerID'].describe()['top']\n",
    "most_freq_customer_counts = df_retail['CustomerID'].describe()['freq']\n",
    "print('most_freq_customer_id:', most_freq_customer_id)\n",
    "print('most_freq_customer_counts:', most_freq_customer_counts)\n",
    "# 8)\n",
    "most_freq_item_sold = df_retail['Description'].describe()['top']\n",
    "stockcode_most_freq_item_sold = df_retail[df_retail['Description']==most_freq_item_sold]['StockCode']\n",
    "num_most_freq_item_sold = df_retail['Description'].describe()['freq']\n",
    "print('stockcode_most_freq_item_sold:', stockcode_most_freq_item_sold)\n",
    "print('num_most_freq_item_sold:', num_most_freq_item_sold)\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "first_transaction_date: 12/01/09 07:46 AM\n",
      "last_transaction_date: 12/09/10 07:28 PM\n",
      "unique_num_items: 2263\n",
      "unique_num_customers: 2181\n",
      "transactions_with_84839: 8\n",
      "max_quantity: 10000\n",
      "Invoice: 6413    502269\n",
      "6414    502269\n",
      "6415    502269\n",
      "6416    502269\n",
      "Name: Invoice, dtype: int64\n",
      "Invoice: 37445    530715\n",
      "Name: Invoice, dtype: int64\n",
      "total_price: 15818.4\n",
      "number_cakestand: 447\n",
      "most_freq_customer_id: 14646\n",
      "most_freq_customer_counts: 422\n",
      "stockcode_most_freq_item_sold: 5659     22423\n",
      "5707     22423\n",
      "5739     22423\n",
      "5782     22423\n",
      "5996     22423\n",
      "         ...  \n",
      "46359    22423\n",
      "46593    22423\n",
      "46834    22423\n",
      "46905    22423\n",
      "46937    22423\n",
      "Name: StockCode, Length: 407, dtype: int64\n",
      "num_most_freq_item_sold: 407\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Discussion (5 Points)\n",
    "\n",
    "In 1.1 we cleaned the dataset of HDB Resale Prices. Now we can use it for any subsequent analysis such as building a regression model to predict the resale prices based on the attributes of flats, but also other analysis such as clustering. Performing a specific analysis (e.g., training a regression model as covered in future lectures) is not part of this assignment! However, any analysis benefits from a very good understanding of the data. This may include the relevance and/or importance of attributes, the types of attributes (e.g., nominal, ordinal, interval, ratio), the need for converting attributes (e.g., via encoding), the need for normalization, important information not captured by the dataset, etc.\n",
    "\n",
    "**Inspect the HDB Resale Price dataset to get a better understand and briefly discuss your findings!** Your finding may cover the issues outlined above (attribute relevance/importance, attribute types, further preprocessing steps, etc.) or anything else you deem relevant.\n",
    "\n",
    "**Your Answer:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. `floor_area_sqm` and `resale_price` are positively correlated.\n",
    "2. The field `lease_commence_date` can be further processed as the age of the house instead of the year the house was built.\n",
    "3. First clarify the meaning of the values of `executive` and `multi generation`, and then convert them into the form of `** room`. Then convert `** room` or `** - room` into the intergers, only keep the number of rooms.\n",
    "4. Maybe the `storey_range` can be reclassified into three categories: {'low', 'middle', 'high'};\n",
    "5. The field `floor_area_sqm` and `resale_price` can be normalized."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "df_hdb.corr()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     floor_area_sqm  lease_commence_date  resale_price\n",
       "floor_area_sqm             1.000000             0.053226      0.628217\n",
       "lease_commence_date        0.053226             1.000000      0.235370\n",
       "resale_price               0.628217             0.235370      1.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053226</td>\n",
       "      <td>0.628217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lease_commence_date</th>\n",
       "      <td>0.053226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resale_price</th>\n",
       "      <td>0.628217</td>\n",
       "      <td>0.235370</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------------------------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Association Rule Mining - Apriori Algorithm (25 Points)\n",
    "\n",
    "Your task is to implement the Apriori Algorihtm for finding Association Rules. In more detail, we focus on the **Apriori Algorithm for finding Frequent Itemsets** -- once we have the Frequent Itemsets, we use a naive approach for the association rule. We will provide the small method for later. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "The following dataset with 5 transaction and 6 different items is directly taken from the lecture slides. This should make it easier to test your implementation. The format is a list of tuples, where each tuple represents the set of items of an individual transactions. This format can also be used as input for the `efficient-apriori` package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Auxiliary Methods\n",
    "\n",
    "We want you to focus on Apriori algorithm. So we provide with a set of auxiliary functions. Feel free to look at their implementation in the file `data/utils.py`.\n",
    "\n",
    "The method `unique_items()` returns all the unique items across all transactions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "unique_items(transactions_demo)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bread', 'cereal', 'cheese', 'eggs', 'milk', 'yogurt'}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method `support()` calculates and return the support for a given itemset and set of transactions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "support(transactions_demo, ('bread','milk'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method `confidence()` calculates and return the confidence for a given association rules and set of transactions. An association rule is represented by a 2-tuple, where the first element represents itemset X and the second element represents items Y (i.e., $X \\Rightarrow Y$)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "confidence(transactions_demo, (('bread',), ('milk',)))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method `generate_association_rules()` calculates and returns all possible association rules given an itemset. The result is a list of association rules, each association rule represented as 2-tuple (see above)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "generate_association_rules(('bread', 'milk', 'cereal'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('bread',), ('cereal', 'milk')),\n",
       " (('cereal',), ('bread', 'milk')),\n",
       " (('milk',), ('bread', 'cereal')),\n",
       " (('bread', 'cereal'), ('milk',)),\n",
       " (('bread', 'milk'), ('cereal',)),\n",
       " (('cereal', 'milk'), ('bread',))]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method `merge_itemsets()` merges two given itemsets into one itemset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "merge_itemsets(('bread', 'milk'), ('bread', 'eggs'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('bread', 'eggs', 'milk')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For your implementation, you can make use of these auxiliary methods wherever you see fit. And that is, of course strongly recommended, as it makes the programming task much easier. So, let's get started."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Generate & Prune (k+1) Candidate Itemsets\n",
    "\n",
    "Let's assume we have found all Frequent Itemsets for size $k-1$. The next is now to find all Candidate Itemets of size $k$. In the lecture we introduced two methods for this. For this assignment, we focus on the $\\mathbf{F_{k-1} \\times F_{k-1}}$ methods -- that is, we use the Frequent Itemsets from the last step to calculate the Candidate Itemsets for the current step\n",
    "\n",
    "Recall that we also can (and should) **prune** any Candidate Itemsets than cannot possible also be Frequent Itemsets  based on the information we already have. In other words, the Candidate Itemsets of size $k$ should only contain the itemsets for which we indeed calculate the support for.\n",
    "\n",
    "**Implement `generate_kplus1_itemsets()` to calculate the Candidate Itemsets of size $k$ given the Frequent Itemsets of size $k-1$!** Note that we walked in detail through an example of this process in the lecture. Below is a code cell that reflects the this example to test your implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def generate_kplus1_itemsets(k_itemsets):\n",
    "    \n",
    "    # Just as fail-safe, return an empty set if the k_itemset is None or empty\n",
    "    if k_itemsets is None or len(k_itemsets) == 0:\n",
    "        return set()\n",
    "    \n",
    "    # It's convenient to actually have the value for k (e.g., k=3 for 3-itemsets)\n",
    "    # The code just looks a bit odd since we cannot get an element from a set using indexing\n",
    "    k = len(next(iter(k_itemsets)))\n",
    "\n",
    "    # Initialize as set to avoid duplicates\n",
    "    kplus1_itemsets = set()\n",
    "    \n",
    "    for itemset1 in k_itemsets:\n",
    "        for itemset2 in k_itemsets:\n",
    "            \n",
    "            ######################################################################\n",
    "            ### Your code starts here ############################################\n",
    "            \n",
    "            # get the merge itemset of itemset1 and itemset2\n",
    "            itemset = merge_itemsets(itemset1, itemset2)\n",
    "\n",
    "            # only keep itemset of length k+1\n",
    "            if len(itemset) == k+1:\n",
    "                # get subsets of itemset of length k\n",
    "                itemset_subset_k = list()\n",
    "                for i in range(len(itemset)):\n",
    "                    temp = list(itemset)\n",
    "                    temp.pop(i)\n",
    "                    itemset_subset_k.append(temp)\n",
    "                \n",
    "                # pruning, delete itemset if any subset of itemset of length k is not in k_itemsets\n",
    "                item_flag = True\n",
    "                for item in itemset_subset_k:\n",
    "                    if tuple(item) not in k_itemsets:\n",
    "                        item_flag = False\n",
    "\n",
    "                if item_flag:\n",
    "                    kplus1_itemsets.add(itemset)\n",
    "            \n",
    "            ### Your code ends here ##############################################\n",
    "            ######################################################################\n",
    "            \n",
    "            pass # Just there so the empty loop does not throw an error\n",
    "    \n",
    "    return kplus1_itemsets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The example below is directly taken from the lecture slides. As such the output should match that of the example as shown on the slides. The input is a set of Frequent Itemsets of size 2, and the output is a set with all Candidate Itemsets of size 3."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "kplus1_itemsets = generate_kplus1_itemsets(\n",
    "    {('bread', 'cereal'), ('bread', 'milk'), ('bread', 'yogurt'), \n",
    "     ('cereal', 'milk'), ('cereal', 'yogurt'), ('milk', 'yogurt')}\n",
    ")\n",
    "\n",
    "\n",
    "for itemset in kplus1_itemsets:\n",
    "    print(itemset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('bread', 'milk', 'yogurt')\n",
      "('bread', 'cereal', 'yogurt')\n",
      "('cereal', 'milk', 'yogurt')\n",
      "('bread', 'cereal', 'milk')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Generate Frequent Itemsets with Apriori Algorithm\n",
    "\n",
    "The method `generate_kplus1_itemsets()` covered the \"Generate\" and \"Prune\" step of the Apriori Algorithm for finding Frequent Itemsets. Now only the \"Calculate\" and \"Filter\" step is missing. However, with `generate_kplus1_itemsets()` in place and together with the auxiliary methods we provide (see above), putting the Apriori Algorithm together should be pretty straightforward.\n",
    "\n",
    "**Implement `frequent_itemsets_apriori()` to find all Frequent Itemset given a set of transactions an a minimum support of `min_support`!** Again, below is a code cell that reflects the this example to test your implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def frequent_itemsets_apriori(transactions, min_support):\n",
    "    # The 1-itemsets are just all unique items across all transactions\n",
    "    one_itemsets = unique_items(transactions)\n",
    "    ############################################################################################\n",
    "    ### Your code starts here ##################################################################\n",
    "    \n",
    "    # Calculate frequent 1-itemsets -- using the auxiliary methods provided, this can be a one-liner :)\n",
    "    frequent_1_itemsets = None\n",
    "    \n",
    "    frequent_1_itemsets = set([(item,) for item in one_itemsets if support(transactions,(item,))>=min_support])\n",
    "    \n",
    "    ### Your code ends here ####################################################################\n",
    "    ############################################################################################\n",
    "    \n",
    "    # Initialize dictionary with all current frequent itemsets for each size k\n",
    "    # Example: { 1: {(a), (b), (c)}, 2: {(a, c), ...} }\n",
    "    frequent_itemsets = { 1: frequent_1_itemsets }\n",
    "    \n",
    "    for k in range(1, len(one_itemsets)+1):\n",
    "\n",
    "        frequent_kplus1_itemsets = set()\n",
    "        \n",
    "        ########################################################################################\n",
    "        ### Your code starts here ##############################################################\n",
    "        \n",
    "        kplus1_itemsets = generate_kplus1_itemsets(frequent_itemsets[k])\n",
    "        frequent_kplus1_itemsets = set([item for item in kplus1_itemsets if support(transactions,item) >= min_support])\n",
    "\n",
    "        ### Your code ends here ################################################################\n",
    "        ########################################################################################\n",
    "                \n",
    "        frequent_itemsets[k+1] = frequent_kplus1_itemsets    \n",
    "\n",
    "    # Merge the dictionary of itemsets to a single set and return it\n",
    "    # Example: {1: {(a), (b), (c)}, 2: (a, c)} => {(a), (b), (c), (a, c)}\n",
    "    return set.union(*[ itemsets for k, itemsets in frequent_itemsets.items() ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, you can check your implementation using the example from the lecture slides."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "frequent_itemsets = frequent_itemsets_apriori(transactions_demo, 0.6)\n",
    "for itemset in frequent_itemsets:\n",
    "    print(itemset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('cereal',)\n",
      "('milk', 'yogurt')\n",
      "('milk',)\n",
      "('cereal', 'milk')\n",
      "('bread', 'yogurt')\n",
      "('bread',)\n",
      "('yogurt',)\n",
      "('bread', 'milk')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### From Frequent Itemsets to Association Rules\n",
    "\n",
    "Your implementation so far gives you the Frequent Itemsets in a list of transactions using the Apriori method. This step is typically the most time-consuming one in Association Rule Mining. However, we still have to do the second step and find all Association Rules given the Frequent Itemsets. We saw in the lecture, that this can also be done in an efficient manner using the Apriori method to avoid checking all rules.\n",
    "\n",
    "Since this step is typically less computationally expensive, we simply do it the naive way -- that is, we go over all Frequent Itemsets, and check for Frequent Itemset and check which of the Association Rules that can be generated from it has a sufficiently high confidence. With all the auxiliary methods we provide, this becomes trivial to implement, so we simply give you the method `find_association_rules()` below. Note how it uses your implementation of `frequent_itemsets_apriori()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def find_association_rules(transactions, min_support, min_confidence):\n",
    "    # Initialize empty list of association rules\n",
    "    association_rules = []\n",
    "    \n",
    "    # Find and loop over all frequent itemsets\n",
    "    for itemset in frequent_itemsets_apriori(transactions, min_support):\n",
    "        if len(itemset) == 1:\n",
    "            continue\n",
    "\n",
    "        # Find and loop over all association rules that can be generated from the itemset\n",
    "        for r in generate_association_rules(itemset):\n",
    "            # Check if the association rule fulfils the confidence requriement\n",
    "            if confidence(transactions, r) >= min_confidence:\n",
    "                association_rules.append(r)\n",
    "                \n",
    "    # Return final list of association rules\n",
    "    return association_rules\n",
    "\n",
    "find_association_rules(transactions_demo, 0.6, 1.0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('cereal',), ('milk',))]"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If everything is correct, for the default values for `min_support` and `min_confidence`, the one Association Rules that should be returned is $\\{cereal\\}\\Rightarrow \\{milk\\}$ (in Python represented as a tuple of 2 tuples, left-hand side and right-hand side)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison with `efficient-apriori` package\n",
    "\n",
    "You can run the apriori algorithm over the demo data to check if your implementation is correct. Try different values for the parameters `min_support` and `min_confidence` and compare the results. Note that the order of the returned association rules might differ between your implementation and the apriori one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, rules = apriori(transactions_demo, min_support=0.6, min_confidence=1.0, max_length=4)\n",
    "\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rule [('cereal',) => ('milk',)] (support: 0.6, confidence: 1.0, lift: 1.25)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `efficient-apriori` provides, of course, a much more efficient and convenient (e.g., keeping track of all the metrics for each rule). And this is why we use this package for finding Association Rules in a real-world dataset below. Still, in its core, `efficient-apriori` implements the same underlying Apriori method to Find Frequent Itemsets (but also to find the Association Rules). If you're interested, at the end, you cam compare the runtimes of `efficient-apriori` and your implementation. Just don't be too disappointed :)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Association Rule Mining over Real-World Datasets (COVID-19) (9 Points)\n",
    "\n",
    "In this task we, use the [Coronavirus Disease 2019 (COVID-19) Clinical Data Repository](https://covidclinicaldata.org/) to find Association Rules that might tell as, which symptoms are most indicative of a COVID-19 infections. We already downloaded, cleaned, and prepared the dataset for you, so you can use it to mine Association Rules.\n",
    "\n",
    "The dataset file `data/a1-covid-symptoms-result.csv` contains over 710k transactions. Each transaction is a set of $0..n$ symptoms and $1$ test result label (\"POSITIVE\" and \"NEGATIVE\"). For example a line in the file can look like `runny_nose sore_throat fatigue POSITIVE`. Note that a line might also be just `NEGATIVE` in case a person was tested without any symptoms. Feel free to take a look at the file -- looking at the raw data is always a good first step when it comes to data mining.\n",
    "\n",
    "#### Loading the Data\n",
    "\n",
    "Since we have transactional data and not tabular-like data, using `pandas` does not really help. We therefore simply read the dataset file line by line to generate our list of transactions. Note that we generate 2 lists of transactions:\n",
    "* `transactions_covid_all` constains all 710k+ transactions in the dataset\n",
    "* `transactions_covid_pos` constains all 11k+ transactions with a \"POSITVE\" test result label"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "transactions_covid_all = []\n",
    "transactions_covid_pos = []\n",
    "\n",
    "with open('data/a1-covid-symptoms-result.csv') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if 'POSITIVE' in line:\n",
    "            transactions_covid_pos.append(line.split(' '))\n",
    "        \n",
    "        transactions_covid_all.append(line.split(' '))\n",
    "\n",
    "print('Number of transactions overall: {}'.format(len(transactions_covid_all)))\n",
    "print('Number of \"POSITIVE\" transactions: {}'.format(len(transactions_covid_pos)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of transactions overall: 710350\n",
      "Number of \"POSITIVE\" transactions: 11060\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compared to traditional Market Basket Analysis where all items in a transaction are of the same type (e.g., products in a supermarket), a COVID data transactions contains both symptoms and test result labels. This means that we might find important Association Rules such as $(\\text{runny_nose} \\Rightarrow \\text{fever})$. These are perfectly valid rules, but in the following, we are interested only in rules where the right-hand side is either \"POSITIVE\" or \"NEGATIVE\".\n",
    "\n",
    "To make this easy for you, we provide a `show_top_rules()` which computes the Association Rules using the `efficient-apriori` package, but (a) filters the rules w.r.t. to the right-hand side, (b) sorts the rules w.r.t. the specified metric, and (c) shows only the top-k rules."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Run the following 4 code cells and interpret the results below!** All 4 code cells find Association Rules using the `efficient-apriori` package encapsulated in the auxiliary method `show_top_rules()` for convenience. Note how they differ with respect to the parameters including the used dataset and the restriction of the right-hand side of the resulting rules!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "%%time\n",
    "# Run A\n",
    "show_top_rules(transactions_covid_all, min_support=0.001, min_confidence=0.2, k=5, sort='lift', rhs='POSITIVE')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Total Number of Rules: 8861 | Number of rules with matching RHS: 4 ===\n",
      "('loss_of_smell', 'loss_of_taste') => ('POSITIVE',): supp: 0.001, conf: 0.296, lift: 19.016\n",
      "('loss_of_smell',) => ('POSITIVE',): supp: 0.002, conf: 0.224, lift: 14.389\n",
      "('cough', 'fever', 'headache') => ('POSITIVE',): supp: 0.001, conf: 0.205, lift: 13.149\n",
      "('loss_of_taste',) => ('POSITIVE',): supp: 0.002, conf: 0.200, lift: 12.866\n",
      "\n",
      "CPU times: user 12.1 s, sys: 89.6 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "# Run B\n",
    "show_top_rules(transactions_covid_pos, min_support=0.15, min_confidence=0.8, k=5, sort='lift', rhs='POSITIVE')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Total Number of Rules: 6 | Number of rules with matching RHS: 6 ===\n",
      "('fatigue',) => ('POSITIVE',): supp: 0.195, conf: 1.000, lift: 1.000\n",
      "('fever',) => ('POSITIVE',): supp: 0.200, conf: 1.000, lift: 1.000\n",
      "('muscle_sore',) => ('POSITIVE',): supp: 0.193, conf: 1.000, lift: 1.000\n",
      "('headache',) => ('POSITIVE',): supp: 0.240, conf: 1.000, lift: 1.000\n",
      "('cough',) => ('POSITIVE',): supp: 0.323, conf: 1.000, lift: 1.000\n",
      "('sore_throat',) => ('POSITIVE',): supp: 0.174, conf: 1.000, lift: 1.000\n",
      "\n",
      "CPU times: user 46.5 ms, sys: 1.82 ms, total: 48.3 ms\n",
      "Wall time: 47.6 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "# Run C\n",
    "show_top_rules(transactions_covid_all, min_support=0.04, min_confidence=0.8, k=5, sort='lift', rhs='NEGATIVE')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Total Number of Rules: 6 | Number of rules with matching RHS: 6 ===\n",
      "('sore_throat',) => ('NEGATIVE',): supp: 0.076, conf: 0.966, lift: 0.981\n",
      "('fatigue',) => ('NEGATIVE',): supp: 0.079, conf: 0.963, lift: 0.978\n",
      "('runny_nose',) => ('NEGATIVE',): supp: 0.044, conf: 0.961, lift: 0.976\n",
      "('headache',) => ('NEGATIVE',): supp: 0.068, conf: 0.948, lift: 0.963\n",
      "('muscle_sore',) => ('NEGATIVE',): supp: 0.042, conf: 0.933, lift: 0.948\n",
      "('cough',) => ('NEGATIVE',): supp: 0.070, conf: 0.933, lift: 0.947\n",
      "\n",
      "CPU times: user 1.86 s, sys: 51.7 ms, total: 1.91 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "# Run D\n",
    "show_top_rules(transactions_covid_all, min_support=0.001, min_confidence=0.8, k=5, sort='lift', rhs='NEGATIVE')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Total Number of Rules: 646 | Number of rules with matching RHS: 345 ===\n",
      "('sore_throat',) => ('NEGATIVE',): supp: 0.076, conf: 0.966, lift: 0.981\n",
      "('fatigue',) => ('NEGATIVE',): supp: 0.079, conf: 0.963, lift: 0.978\n",
      "('sob',) => ('NEGATIVE',): supp: 0.035, conf: 0.962, lift: 0.977\n",
      "('runny_nose',) => ('NEGATIVE',): supp: 0.044, conf: 0.961, lift: 0.976\n",
      "('fatigue', 'sob') => ('NEGATIVE',): supp: 0.017, conf: 0.961, lift: 0.976\n",
      "('runny_nose', 'sore_throat') => ('NEGATIVE',): supp: 0.017, conf: 0.958, lift: 0.973\n",
      "\n",
      "CPU times: user 12.8 s, sys: 124 ms, total: 13 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3a) Discuss your obervations! (3 Points)** You must have noticed numerous differences between the 4 runs A-D. List at least 3 differences you have found. You may want to consider the elapsed time and the quality of the results. Briefly explain your observations!\n",
    "\n",
    "**Your Answer:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "1. Both `min_support` and `min_condifence` have influences on the result of the number of rules. The smaller the `min_support`, the more rules. \n",
    "2. Compare A and D, with the same `min_support` and different `min_confidence`, there is not much difference in cpu time. While comparing B and C, withe the same `min_confidence` and different `min_support`, there is a big difference in cpu time, the larger `min_support` takes less cpu time.\n",
    "3. When the `min_support` is greater than 0.04, the number of rules remain the same as 6. \n",
    "4. The lift of `positive` is greater than 1, and the lift of `negative` is smaller than 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3b) Interpret the results! (3 Points)** Runs A and B return association rules with symptoms on the left-hand side and a POSITIVE test result on the right-hand side. As such both runs find rules which (combination of) symptoms are most indicative of a positive test result. However, the results of Run A and B a rather different. Explain the differences and discuss which result provides more reliable insights!\n",
    "\n",
    "**Your Answer:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result is different due to the choice of the parameters `min_support` and `min_confidence`. The `min_support` of round A is much smaller than that of round B, therefore, the number of total rules of round A is significantly larger than that of round B.  \n",
    "I think the result A is more reliable. From the result we can see that, the `min_confidence` in round B is too large that only itemset with one item can meet the condition. Although the result of round B has higher support and confidence, the lift is only 1. While the lift in round A is very obvious, with the highest lift of 19.06. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3c) Disccus effects of input parameters! (3 Points)** From your observation, what are the effects of increasing/reducing `min_support` and `min_confidence`? In which cases (i.e., runs A-B) we can optimize the parameters for performance without losing any quality in the results. Support your answer with evidence. You can perform more runs with different parameter settings, if needed.\n",
    "\n",
    "**Your Answer:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The chart below shows the result of rounds I performed.\n",
    "| Round | min_support | min_confidence | total rules | match rules |  cpu time   |\n",
    "|-------|-------------|----------------|-------------|-------------|-------------|\n",
    "| 1     |     0.001   |      0.02      |     18359   |     35      |    12.9 s   |\n",
    "| 2     |     0.04    |      0.02      |      12     |     0       |    1.97 s   |\n",
    "| 3     |     0.15    |      0.02      |      0      |     0       |    362 ms   |\n",
    "| 4     |     0.001   |      0.2       |     8861    |     4       |    12.8 s   |\n",
    "| 5     |     0.04    |      0.2       |     6       |     0       |    1.9 s   |\n",
    "| 6     |     0.15    |      0.2       |     0       |     0       |    353 ms   |\n",
    "| 7     |     0.001   |      0.8       |      646    |     0       |    12.1 s  |\n",
    "| 8     |     0.04    |      0.8       |      6      |     0       |    2.08 s   |\n",
    "| 9     |     0.15    |      0.8       |      0      |     0       |    340 ms   |\n",
    "\n",
    "1. Comparing round 1 with round 2 and round 3,or comparing round 4 with round 5 and round 6, or comparing round 7 with round 8 and round 9, when the `min_confidence` is the same, the increase of `min_support` will reduce the number of total rules, and increase the computation time.\n",
    "2. When the `min_confidence` is greater than a certain threshold (which is unknown in this experiment), the increase of `min_confidence` will reduce the number of total rules, otherwise the number of rules remains the same with the increse of `min_confidence`.\n",
    "3. The cpu time is mainly affected by the value of `min_support`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Comparison with Different Use Case (Market Basket Analysis) (6 Points)\n",
    "\n",
    "The COVID-19 dataset contains 710k+ transactions. Now let's make a basic comparison with a different dataset. The [Online Retail II](https://archive.ics.uci.edu/ml/machine-learning-databases/00502/) dataset contains all the transactions occurring for a UK-based and registered, non-store online retail. The company mainly sells unique all-occasion giftware. Many customers of the company are wholesalers.\n",
    "\n",
    "We already downloaded and prepared the dataset for you to be find Association Rules. The file `data/a1-retail-transactions.csv` contains ~19.8k transactions; each line represents one transaction. Each transaction is a set of product codes; for this task, we don't need to know what the actual products are (but if you're curious, you can check with the original dataset linked above)\n",
    "\n",
    "Let's read the dataset file with retail transactions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "transactions_retail = []\n",
    "\n",
    "with open('data/a1-retail-transactions.csv') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        transactions_retail.append(line.split(' '))\n",
    "\n",
    "print('Number of retial transactions: {}'.format(len(transactions_retail)))\n",
    "print('Example transactions: {}'.format(transactions_retail[0]))\n",
    "\n",
    "print(len(unique_items(transactions_retail)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of retial transactions: 19853\n",
      "Example transactions: ['85048', '79323P', '79323W', '22041', '21232', '22064', '21871', '21523']\n",
      "4094\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's use `efficient-apriori` to find interesting Assication Rules."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "%%time \n",
    "\n",
    "_, rules_retail = apriori(transactions_retail, min_support=0.01, min_confidence=0.2)\n",
    "\n",
    "print('Overall number of rules: {}'.format(len(rules_retail)))\n",
    "\n",
    "# Let's sort w.r.t. to lift\n",
    "rules_retail = sorted(rules_retail, key=lambda rule: rule.lift, reverse=True)\n",
    "\n",
    "# Print the top-5 rules w.r.t to lift\n",
    "for i, r in enumerate(rules_retail):\n",
    "    # Stop after i rules\n",
    "    if i >= 5:\n",
    "        break\n",
    "    \n",
    "    print('Rule [{} => {}] (support: {:.4f}, confidence: {:.4f}, lift: {:.4f})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overall number of rules: 638\n",
      "Rule [('22748',) => ('22745',)] (support: 0.0112, confidence: 0.7957, lift: 63.4418)\n",
      "Rule [('22745',) => ('22748',)] (support: 0.0112, confidence: 0.8916, lift: 63.4418)\n",
      "Rule [('22301',) => ('22300',)] (support: 0.0100, confidence: 0.7425, lift: 56.0517)\n",
      "Rule [('22300',) => ('22301',)] (support: 0.0100, confidence: 0.7567, lift: 56.0517)\n",
      "Rule [('22699',) => ('22697',)] (support: 0.0108, confidence: 0.7329, lift: 55.7464)\n",
      "CPU times: user 3min 14s, sys: 885 ms, total: 3min 15s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.4a) Discuss your observations! (4 Points)** If you compare with the Runs A-D in 2.3, you should observe several differences when extracting Association Rules from this dataset with retail transactions. List your observations and briefly provide an explanation for each observation. You may want to consider the size of the dataset, the runtimes of `efficient-apriori`, the values of the different metrics for the top rules, etc.\n",
    "\n",
    "\n",
    "**Your Answer:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. The size of retail dataset is only 1.8 times that of the Covid-19 dataset, while the running time is dozens of times longer than the latter. When applying the efficient-apriori algorithm, calculation time doubles with the amount of data.\n",
    "2. The top rules of the retail dataset are all one single item itemset, while the match rules of the Covid-19 dataset contains itemset which has more than two itmes. I think this is very reasonable. When several symptoms appear together, we can infer that the Covid-19 result is positive. However, when shopping, the probability of buying two items together is relatively high, and it is not common to buy multiple items together.\n",
    "3. Compared to Runs A in 2.3, the confidence and lift of the retail dataset are both significantly higher than that of Covid-19 dataset. I think this is because a positive Covid-19 result is a rare event with a low probability, while buying two products together is very common. Therefore, the confidence and lift are higher in retail dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.4b) Perform a Complexity Analysis of the Brute-Force Approach! (2 Points)**. We know from the lecture that the brute-force implementation for Frequent Itemset Generation has to check $2^d-1$ itemsets, with $d$ being the number of unique items. Suppose we can count 2^{36} itemsets per second. How many unique items (approx.) may a transaction dataset contain so that we will be able to complete the counting before the sun burns out (the sun has another $5\\cdot 10^9$ years to burn)? And what does it mean for our COVID-19 and our Retail Dataset?\n",
    "\n",
    "\n",
    "**Your Answer:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the equation`2^(d)-1 = 5*10^(9)*365*24*60*60*2^(36)`, we can know `d` is approximately equal to `93`. As we know the unique items of Covid-19 is 4094, which means it will take forever to count all unique items using brute force. It is a completely impossible task!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------------------------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing Your Implementation with `efficient-apriori` (just for fun!)\n",
    "\n",
    "So far, you run and tested your implementation for finding Association Rules (focus on the Frequent Itemsets) on the toy dataset from the lecture. Now let's run it on the COVID dataset. However, we better just use a sample of 10% for this :)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transactions_covid_sample = []\n",
    "\n",
    "with open('data/a1-covid-symptoms-result-sample.csv') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        transactions_covid_sample.append(line.split(' '))\n",
    "\n",
    "print('Number of transactions overall: {}'.format(len(transactions_covid_sample)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's find all Association Rules using your implementation and with the same parameters as for Run A (see above). Depending on your machine, this may take a couple of minutes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "association_rules = find_association_rules(transactions_covid_sample, 0.001, 0.2)\n",
    "\n",
    "relevant_rules = [ r for r in association_rules if r[1][0] == 'POSITIVE' ]\n",
    "\n",
    "for r in relevant_rules:\n",
    "    print(r)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hopefully, your implementation returns the same 4 Association Rules as `efficient-apriori` for Run A :). Apart from that, you cannot fail to observe the difference in performance. However, performance and optimization was not the focus here (this includes the rather naive implementation of the auxiliary methods we provided). This comparison should help you to appreciate the complexity of the task of Association Rule Mining over large datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('deeplearn_course': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "55f4922a8994d14ca337312ca5de846259fa9341ac8218d53e37a4a1608cd206"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}