{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6ca913",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 1a - EDA & DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fe03b",
   "metadata": {},
   "source": [
    "Hello everyone, this assignment notebook covers Exploratory Data Analysis (EDA) and DBSCAN. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A1a_YourName_YourNUSNETID.ipynb** (e.g., **A1a_BobSmith_e12345678.ipynb**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Remember to rename and save the script file *A1a_script.py* as **A1a_YourName_YourNUSNETID.py** (e.g., **A1a_BobSmith_e12345678.py**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Submission deadline is Sep 11, 11.59 pm (together with A1b). Late submissions will be penalized by 10% for each additional day.\n",
    "\n",
    "Please also add your nusnet and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cae836",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0250637E'\n",
    "nusnet_id = 'E0945819'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479007c",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Data Cleaning & Exploratory Data Analysis (EDA) (30 Points)**\n",
    "    * 1.1 Preparing a Real-World Dataset for Analysis (22 Points)\n",
    "        * 1.1 a) Removing \"Dirty\" Records (6 Points)\n",
    "        * 1.1 b) Handling Missing (NaN) Values (6 Points)\n",
    "        * 1.1 c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "        * 1.1 d) Handling of Categorical Attributes (4 Points)\n",
    "    * 1.2 Basic Facts about a Real-World Dataset (8 Points)\n",
    "* **2 Identifying Noise Using Clustering (20 Points)**\n",
    "    * 2.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "        * 2.1 a) Compute Core Points (5 Points)\n",
    "        * 2.2 b) Compute Noise Points (5 Points)\n",
    "    * 2.2 Questions about DBSCAN (10 Points)\n",
    "        * 2.2 a) `get_noise_dbscan()` vs. `sklearn.cluster.DBSCAN` (2 Points)\n",
    "        * 2.2 b) Basic Data Understanding with DBSCAN (3 Points)\n",
    "        * 2.2 c) Effects of Data Manipulation on DBSCAN Results (2 Points)\n",
    "        * 2.2 d) Identifying Noise/Outliers with Clustering beyond DBSCAN (3 Points)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ffdec",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa18ac",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A1a_script.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from A1a_SONGRENHE_e0945819 import get_noise_dbscan\n",
    "#from A1a_BobSmith_e12345678 import get_noise_dbscan # <-- you well need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38541ac6",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dead6f",
   "metadata": {},
   "source": [
    "# 1 Data Cleaning & Explorative Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85572cfd",
   "metadata": {},
   "source": [
    "### 1.1 Cleaning a Real-World Dataset (XX Points)\n",
    "\n",
    "For the following tasks, we consider a dataset containing information 20,000 past resale transactions of condo flats. Each record (i.e., data samples) consists of 12 attributes. The following **data description** list all attributes together with a brief description of each attribute's data type / domain:\n",
    "\n",
    "* **transaction_id**: Unique ID of the resale transactions; an 8-digit integer number uniquely assigned to each transaction.\n",
    "* **url**: Unique link to a website documenting this transaction as a string value.\n",
    "* **name**: The name of the condo as a string value (e.g., \"estella gardens\", \"eedon green\").\n",
    "* **type**: The type of condo as string value (e.g., \"condominium\", \"apartment\").\n",
    "* **postal_district**: The postal district the condo is located in as integer value; Singapore has 28 postal districts: 1, 2, ..., 28 (cf. [here](https://www.ura.gov.sg/realEstateIIWeb/resources/misc/list_of_postal_districts.htm)).\n",
    "* **subzone**: The subzone the condo is located in as a string value.\n",
    "* **planning_area**: The planning area the condo is located in as a string value.\n",
    "* **region**: The region the condo is located in as a string value.\n",
    "* **date_of_sale**: The date (month & year) of the transaction as a string value (e.g., \"mar-19\", \"oct-20\").\n",
    "* **area_sqft**: The size of the condo flat in square feet as a positive integer value.\n",
    "* **floor_level**: The range of floors in which the flat is located in the condo as string value (e.g., \"06 to 10\", \"11 to 15\").\n",
    "* **eco_category**: The eco category of the condo as a single-character string value (e.g., \"A\", \"B\", \"C\", \"D\").\n",
    "* **price**: Resale price of the condo flat in Singapore Dollar as an integer value.\n",
    "\n",
    "Additional information: Singapore has 55 panning areas; each planning area is further split into multiple subzones (if you are interested, you can check out the corresponding [Wikipedia article](https://en.wikipedia.org/wiki/List_of_places_in_Singapore)).\n",
    "\n",
    "**Important:** In each of the following subtask 1.1 a)-c) we use a slightly different version of the dataset. This allows you to focus on the specific aspects of data cleaning / data preprocessing addresses in the respective subtask. In practice, of course, there would be only one dataset containing all the different issues (e.g., \"dirty\" records, missing data, etc.) at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e96b7",
   "metadata": {},
   "source": [
    "#### 1.1 a) Removing \"Dirty\" Records (6 Points)\n",
    "\n",
    "We argued in the lecture that almost all real-world datasets contain some form of noise that might negatively affect any applied data analysis. The very first -- and in some sense -- easiest way to identify noise is to check if all data confirms with the data description. The following code cell shows a snippet of the dataset which you will be looking at in this subtask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_dirty = pd.read_csv('data/a1-condo-resale-dirty.csv')\n",
    "\n",
    "df_condo_dirty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f68ad",
   "metadata": {},
   "source": [
    "If you check the dataset against its description, you will notice that many records are \"dirty\". We define a record as \"dirty\" if it does not adhere to the given data description (see above). Such records are not guaranteed to be valid and should therefore not be used for any analysis.\n",
    "\n",
    "**Identify at least 3 causes of \"dirty\" records and remove all corresponding records from the dataset!** Please provide your answer in the markdown cell below. Additional (simplifying) guidelines:\n",
    "\n",
    "* Ignore missing (`NaN`) values -- that is, a record containing one or more missing values does not make this record dirty. We look at missing values in a subsequent task as their handling depends on the data analysis to be performed.\n",
    "* Ignore the correctness of string values -- that is, we do not expect you to check, e.g., if a street name contains a typo or a planning area is indeed one of the existing 55 planning areas in Singapore\n",
    "\n",
    "**Your Answer:**",
    "\n",
    "transaction_id contains non-numerical character X\n",
    "\n",
    "postal_district not between 1-28, e.g. postal_district = 0\n",
    "\n",
    "non-positive area_sqft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb36c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c2faf0c",
   "metadata": {},
   "source": [
    "Use the code cell below to actually implement your steps for removing the \"dirty\" records. The results should back up your answer above. Feel free to split the cell into multiple code cells to improve organization (not a must, though).\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete -- `pandas` is really powerful and should be your best friend here. If you use loops but the results are correct, there will be some minor deduction of points. But note that it's of course better to have a working solution using loops than having no solution at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_cleaned = df_condo_dirty.copy()\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "df_cleaned = df_condo_dirty[df_condo_dirty['area_sqft'] > 0]\n",
    "df_cleaned = df_cleaned[df_cleaned['postal_district'] > 0]\n",
    "df_cleaned = df_cleaned[~df_cleaned['transaction_id'].str.contains('X')]\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "print('After cleaning, there are now {} records.'.format(df_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cb155",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after the cleaning step as there is some wiggle room regarding the performed steps which would affect this result. As such, even if two solutions are correct, they do not necessarily yield the same number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bac07c",
   "metadata": {},
   "source": [
    "#### 1.1 b) Handling Missing (NaN) Values (6 Points)\n",
    "\n",
    "Many to most traditional data mining algorithms do not like missing (NaN) values and will throw an error if missing values are present. We therefore have to address missing values and get rid of them. On the other hand, we want to preserve as much of our dataset as possible, so we need to be smart about that. In this subtask, you are provided with a version of our condo resale dataset that contains missing values but is otherwise clean -- so it is all about the `NaN` values here.\n",
    "\n",
    "Let's load the dataset and have a quick look -- the attributes are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_nan = pd.read_csv('data/a1-condo-resale-nan.csv')\n",
    "\n",
    "df_condo_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d558f5",
   "metadata": {},
   "source": [
    "Since your decision for handling `NaN` values might depend in the data mining task, assume in the following that you want to use this dataset to **create a regression model to predict the resale price** from the attributes of a transaction. Of course, there will be no need to actually create such a model here :).\n",
    "\n",
    "**Identify all `NaN` values in the dataset and handle them appropriately!** After this preprocessing, the resulting dataset should no longer contain any `NaN` values. Please provide your answer in the markdown cell below list all issues concerning `NaN` values and how you would handle them with a brief justification for your approach. Additional (simplifying) guidelines:\n",
    "\n",
    "* The goal is to preserve as many records in the dataset as possible.\n",
    "* You do not need to consider external knowledge (i.e., information coming from outside this dataset)\n",
    "* Ignore more sophisticated solutions such as [`sklearn.impute.KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html). These can be very useful in practice (and maybe for your project), but their application requires certain assumptions to hold for good results. This is beyond the scope of this assignment.\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "If there are not too many data points with NA values for some attributes, it's often the most straightforward way to simple remove such data points. Replacing/filling NA values is not always applicable and generally make some assumptions that must hold.\n",
    "\n",
    "URL is not required for analysis, so we can just drop this column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d28cdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0be8e11",
   "metadata": {},
   "source": [
    "Use the code cell below to actually implement your steps for handling `NaN` values. The results should back up your answer above. Feel free to split the cell into multiple code cells to improve organization (not a must, though).\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete -- pandas is really powerful and should be your best friend here. If you use loops but the results are correct, there will be some minor deduction of points. But note that it's of course better to have a working solution using loops than having no solution at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_no_nan = df_condo_nan.copy()\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "df_no_nan = df_no_nan.drop(columns=['url'])\n",
    "df_no_nan = df_no_nan.dropna()\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "print('After handling missing values, there are now {} records.'.format(df_no_nan.shape[0]))\n",
    "print('Number of records with an NaN for any attribute: {}'.format((df_no_nan.isna().sum(axis=1) > 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d529c",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after this preprocessing step as there is some wiggle room regarding the performed steps which would affect this result. However, the number of records with `NaN` values should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caddf10",
   "metadata": {},
   "source": [
    "#### 1.1 c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "\n",
    "Identifying \"dirty\" records and missing data are two very fundamental and generally rather systematic steps as part of data cleaning / data preprocessing. However, as we saw in the lecture using some examples, there are many other issues with the dataset that can be considered noise and thus potentially negatively affecting any data analysis. So the more noise we can remove, the more likely we can expect meaning analysis results.\n",
    "\n",
    "For this subtask, we use a version of our condo resale dataset **with no \"dirty\" records or missing data**! Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407dd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_others = pd.read_csv('data/a1-condo-resale-others.csv')\n",
    "\n",
    "df_condo_others.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654ee93",
   "metadata": {},
   "source": [
    "**List all data cleaning / data preprocessing steps you deem appropriate to apply to the dataset above!** Please provide your answer in the markdown cell below list all steps together with a justification for your decision. Additional (simplifying) guidelines:\n",
    "\n",
    "* You should still assume that we want to use this dataset to create a model for predicting the resale price of a flat based on its attributes. The choice of data mining task is very likely to affect your decision for what cleaning / preprocessing steps to apply.\n",
    "* There is no need to consider external knowledge. For example, you do not have to check if a value for `subzone` is indeed an existing subzone of Singapore.\n",
    "* Please do not address the concept of attribute encoding -- that is, the encoding of categorical attributes as numerical attributes -- in this subtask.\n",
    "* There is no need for you to implement any processing steps! Most important are your justifications for your decisions.\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "it's relatively easy to see the area_sqft=1 represents some kind of noisy value. Firstly, it far off the expected distribution, and secondly, we semantically know that a condo cannot have a area_sqft of one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e311bc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663872f4",
   "metadata": {},
   "source": [
    "#### 1.1 d) Handling Categorical Attributes (4 Points)\n",
    "\n",
    "Many to most data mining algorithms require all input features / attributes to be numerical. Our dataset with transactions resales of condo flats contains attributes that are not all numerical. As such, assuming we indeed want to utilize them, we need to convert those attributes into numerical ones. Regarding encoding techniques, we covered [One-Hot Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) in the lecture, and also talked about [Target Encoding](https://contrib.scikit-learn.org/category_encoders/targetencoder.html) in the tutorial; you can and should also look into other encoding techniques.\n",
    "\n",
    "**Briefly(!) discuss how you would handle each categorical attribute and justify why!** Handling a categorical attribute means\n",
    "* to drop a categorical attribute *or*\n",
    "* to encode it into a numerical representation *or*\n",
    "* to otherwise transform it into a numerical attribute\n",
    "\n",
    "There is no single correct answer for this task; it's your justification that matters. Again, assume that we want to create a regression model to predict the resale price of a flat based on the other features.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf85c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e53a69",
   "metadata": {},
   "source": [
    "### 1.2 Basic Facts about a Real-World Dataset (8 Points)\n",
    "\n",
    "The following tasks are about getting basic insights into the Condo Resale Prices dataset. As the data preprocessing steps you choose to perform might affect the results of this task, we will use a modified version here. Note that this version contains 50,000 listing of condo resale transactions and does **not** contain any \"dirty\" records. This is to ensure that everyone uses the same data. This helps marking your solutions as we know which results to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_facts = pd.read_csv('data/a1-condo-resale-facts.csv')\n",
    "\n",
    "df_condo_facts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbfe24",
   "metadata": {},
   "source": [
    "Please complete the table below by answering the 7 given questions. Use the code cell below the table to actually implement your steps that enabled you to answer the questions. There is no need for a fancy layout for any print statement; it's only important that the result is clear.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff81b3",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)-(7). Answers (1)-(6) are worth 1 Point each; Answer (7) is worth 2 Points.\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| (1)  | What is the date (month & year) of the first transactions? | ??? |\n",
    "| (2)  | For each `type`, how many transactions are in the dataset?  | ??? |\n",
    "| (3)  | How many condo apartments in Redhill (*subzone*) costing more than SGD 5,000,000 have been sold? | ??? |\n",
    "| (4)  | What is the planning area with the most transactions? List the name of the planning area and the number of transactions!  | ??? |\n",
    "| (5)  | Which transaction in postal district 11 had the highest price-to-area ratio (i.e., the highest price per square foot)? List the name of the condo and the price per square foot (rounded to 2 decimals)| ??? |\n",
    "| (6)  | What is the correlation between the resale *price* and *area_sqft*? | ??? |\n",
    "| (7)  | What is the number of transactions where the flat/apartment was between the 50th floor (inclusive) and 60th (inclusive) floor?  | ??? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323314b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c19299",
   "metadata": {},
   "source": [
    "## 2 Identifying Noise Using Clustering\n",
    "\n",
    "In Section 1 we focused on addressing any \"obvious\" noise a given dataset may contain. Obvious noise is not well defined, but in general it's this kind of noise that can be identified by having a good look at the data or by means of a simple analysis (e.g., looking at the domains of attributes, or simple statistics such as the distribution/histogram). However, as we saw in the lecture, that some noise is much more difficult to detect. More specifically, we saw that an outlier can indeed only be identified as such when looking at the combination of different data points. Looking at individual attributes / features does not suffice here.\n",
    "\n",
    "If you recall, an outlier refers to some record (i.e., data samples) is very different compared to many to most other records. There are different ways to address this problem, and in this section we see how we can utilize DBSCAN for this task. As the name already indicates, DBSCAN (Density-Based spatial Clustering of Applications with Noise), is a clustering algorithm that has an explicit notion of noise points, i.e., data points that are noticeable more dissimilar to data points that form clusters.\n",
    "\n",
    "As our condo dataset contains too many non-numerical attributes, we use in this section simple datasets (i.e., small datasets containing only numerical attributes). We start with a toy dataset -- 70 records, 2 attributes -- to motivate the idea and for you to test your implementation. Limiting ourselves to 2 attributes allows also for simple visualizations.\n",
    "\n",
    "First, let's read the toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d293427",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a1-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb7738",
   "metadata": {},
   "source": [
    "Now we can run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset. Here we use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=10).fit(X_dbscan_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29407eb1",
   "metadata": {},
   "source": [
    "The points that are noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_point_indices = np.argwhere(dbscan_clustering.labels_ < 0).squeeze()\n",
    "\n",
    "print('The indices of the points labeled as noise are: {}'.format(noise_point_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48a99c",
   "metadata": {},
   "source": [
    "Of course, we can also plot the results. Note that the figure below only highlights the points labeled as noise in red; all points belonging to *some* clusters are in grey (note that we do not care to which exact cluster these points belong).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[:,0], X_dbscan_toy[:,1], c='grey')\n",
    "plt.scatter(X_dbscan_toy[noise_point_indices,0], X_dbscan_toy[noise_point_indices,1], c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615b2ed",
   "metadata": {},
   "source": [
    "Summing up, the red dots in the plots we define as noise or outliers as they are very dissimilar to the other data points. In practice, we would likely remove those noise points, treat them separately, or maybe perform additional preprocessing steps to potentially \"denoise\" the dataset. However, the steps of choice generally depend heavily on the exact data mining task. Here, we focus on the identification of noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251e819",
   "metadata": {},
   "source": [
    "### 2.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "\n",
    "In the lecture, we covered the original algorithm of DBSCAN, which you can also find on [Wikipedia](https://en.wikipedia.org/wiki/DBSCAN). While not difficult to implement, it takes quite a couple of lines of codes to do so. For this assignment, however, we are only interested in the points of a dataset that DBSCAN considers noise (as illustrated above; the red dots in the previous plot). This includes that we do not have to care about\n",
    "\n",
    "* how many clusters there are (the plot above hints at 3 clusters but it does not matter) *and*\n",
    "* which non-noise points (the grey dots in the plot above) belong to which cluster\n",
    "\n",
    "**Your task is to implement a modified/simplified version of DBSCAN to find all noise points in a dataset!** The skeleton of method `get_noise_dbscan()` you need to complete is found in the file `A1a_script.py` (before the apprpriate renaming). The method takes data matrix `X` as well as the two basic parameters `eps` and `min_samples` as input parameters; we use the same naming as scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  The output should be 2 lists of indices: (a) one containing the indices of all *core points* and (b) on containing the indices of all *noise points* in input dataset X.\n",
    "\n",
    "**Important:**\n",
    "* We only split this task into 2.1 a) and 2.1 b) to have intermediate results you can check for correctness (and potentially to better allow for partial marking). Our reference solutions first finds all core points and uses this information to find all noise points; hence the 2 separate code blocks for you to complete.\n",
    "* However, if you have a better/faster/shorter/cooler/etc. solution, you are more than welcome to implement it and ignore the intermediate result of finding all core points. Only the result from 2.1 b) is important. This also means that you can ignore 2.1 a) and still get full marks if you correctly identify all noise points.\n",
    "* If you have an alternative solution, please make sure that the method still returns the 2 output parameters `(core_point_indices, noise_point_indices)`. If you do not need to explicitly identify the core points, you can simply return `None` for `core_point_indices`.\n",
    "* You can import any method `numpy`, `scipy`, `sklearn`, or `pandas` has to offer -- except for any ready-made implementation of DBSCAN, of course :). Please add any imports to the code cell at the top with the other imports. Hint: We already imported [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) for you.\n",
    "\n",
    "**Little competition:** As we expect different implementations here -- and we do not think our solution to be that great anyway :) -- we will run a little competition: We will run each solution against a large(r) dataset and measure the runtime of each solution to get a ranking who's implementation runs fastest. Please note that this ranking is purely bragging rights and will not affect the marks! It's just for a little more incentive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb8bd7",
   "metadata": {},
   "source": [
    "#### 2.1 a) Compute Core Points (5 Points)\n",
    "\n",
    "As mentioned above, our reference solution first computes all core points. If you follow this approach, complete the respective part in the code of method `get_noise_dbscan()`. Some hints:\n",
    "* Recall that we do not care to which cluster a core point a data sample belongs, only that is a core point in *some* cluster\n",
    "* Have a look at method [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html); it might make your life easier.\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete. If you use loops but the results are correct, there will be some minor deduction of points. Once you know what you need to do, it is almost guaranteed that `numpy`, `scipy`, or `sklearn` will provide a useful method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_core_point_indices, _ = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of core points: {}\\n'.format(len(my_core_point_indices)))\n",
    "print('The first 25 indices of the points labeled as core points:\\n{}'.format(sorted(my_core_point_indices)[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c13be",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "    \n",
    "```\n",
    "Total number of core points: 50\n",
    "\n",
    "The first 25 indices of the points labeled as core points:\n",
    "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n",
    "```\n",
    "\n",
    "Note that `0`, `4`, and `27` are missing from this list since [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) told us that these points are noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d097436",
   "metadata": {},
   "source": [
    "#### 2.2 b) Compute Noise Points (5 Points)\n",
    "\n",
    "Knowing the core points is useful but only an intermediate step. Now it is time to complete the method `get_noise_dbscan()` to compute the indices of all noise points in `X`. Again, our reference solution uses `core_point_indices` to accomplish this. If your implementation does not require the information about core points but returns the correct `noise_point_indices` then this is perfectly fine!\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete. If you use loops but the results are correct, there will be some minor deduction of points. Once you know what you need to do, it is almost guaranteed that `numpy`, `scipy`, or `sklearn` will provide a useful method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc974807",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, my_noise_point_indices = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of noise points: {}\\n'.format(len(my_noise_point_indices)))\n",
    "print('The indices of all points labeled as noise points:\\n{}'.format(sorted(my_noise_point_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02edbe1e",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "\n",
    "```\n",
    "Total number of noise points: 10\n",
    "\n",
    "The indices of all points labeled as noise points:\n",
    "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n",
    "```\n",
    "\n",
    "Since we used the same values for `eps` and `min_samples`, this result matches the output we saw earlier when we used scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over the toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caec7b",
   "metadata": {},
   "source": [
    "**Additional tests (nothing of you to do here).** With the code cell below, you can further test your implementation. It uses the popular [IRIS Dataset](https://archive.ics.uci.edu/ml/datasets/iris). It contains 150 data points with 4 numerical features. The code cell extracts the noise points using the DBSCAN implementation of scitkit-learn as well as your own implementation. The two results should always match for the same values of `eps` and `min_samples` for the given examples. There is nothing for you to implement here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris = pd.read_csv('data/a1-iris.csv')[['sepal_length','sepal_width','petal_length','petal_width']].to_numpy()\n",
    "\n",
    "eps, min_samples = 0.6, 6\n",
    "#eps, min_samples = 0.5, 8\n",
    "#eps, min_samples = 0.4, 5\n",
    "\n",
    "sk_clustering_iris = DBSCAN(eps=eps, min_samples=min_samples).fit(X_iris)\n",
    "sk_noise_iris = np.argwhere(sk_clustering_iris.labels_ < 0).squeeze()\n",
    "\n",
    "_, my_noise_iris = get_noise_dbscan(X_iris, eps=eps, min_samples=min_samples)\n",
    "\n",
    "print('sklearn solution: {}'.format(sk_noise_iris))\n",
    "print('My own solution:  {}'.format(my_noise_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da4b57",
   "metadata": {},
   "source": [
    "### 2.2 Questions about DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c98916",
   "metadata": {},
   "source": [
    "#### 2.2 a) `get_noise_dbscan()` vs. `sklearn.cluster.DBSCAN` (2 Points)\n",
    "\n",
    "Above, we used scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) to check the correctness of your implementation of `get_noise_dbscan()` and for the given example parameters (e.g., `eps, min_samples = 0.6, 6`) the results should indeed be identical. \n",
    "\n",
    "Now try `eps, min_samples = 0.3, 3` and compare the result; use the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479738b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, min_samples = 0.3, 3\n",
    "\n",
    "sk_clustering_iris = DBSCAN(eps=eps, min_samples=min_samples).fit(X_iris)\n",
    "sk_noise_iris = np.argwhere(sk_clustering_iris.labels_ < 0).squeeze()\n",
    "\n",
    "_, my_noise_iris = get_noise_dbscan(X_iris, eps=eps, min_samples=min_samples)\n",
    "\n",
    "print('sklearn solution: {}'.format(sk_noise_iris))\n",
    "print('My own solution:  {}'.format(my_noise_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fadf10",
   "metadata": {},
   "source": [
    "**Explain why you see different results for some values for `eps` and `min_samples`?** Even if you think that your implementation of method `get_noise_dbscan()` might contain little errors, you can simply assume here that it is correct and that the results would match the one of `sklearn.cluster.DBSCAN` for the examples in previous code cell (e.g. `eps, min_samples = 0.6, 6`).\n",
    "\n",
    "Hint: Recall from the lecture that DBSCAN is deterministic with respect to the core points and the noise points; only the border points might differ in some cases. However, this statement relies on an assumption that does not (completely) hold on this IRIS dataset.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255b906",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0973b23",
   "metadata": {},
   "source": [
    "**2.2 b) Basic Data Understanding with DBSCAN (3 Points)**\n",
    "\n",
    "Assume you have a dataset `X`, run DBSCAN, and get a clustering that contains a set of clusters and some noise points (there's no need to be more precise; it's only important that you don't get just noise). Let's also assume you create a new dataset `X_new` simply by shuffling `X`; no other changes. Now you run DBSCAN with the same parameters as before over `X_new` and get a different clustering, i.e., most of the clusters are not exactly the same as before.\n",
    "\n",
    "**Describe what this information tells about the dataset and clustering!** This may include a brief discussion how changing the parameters of DBSCAN will likely affect the results.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d8f21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f68dfbe",
   "metadata": {},
   "source": [
    "#### 2.2 c) Effects of Data Manipulation on DBSCAN Results (2 Points)\n",
    "\n",
    "Assume you have a $d$-dimensional dataset `X` in the Euclidean space, i.e., each data point as $d$ numerical features (with each feature value in the interval $[0, 1]$). After running DBSCAN over `X`, you get some clustering (again, we only assume it's not only noise). Now you create a new dataset `X_new` by multiplying all data points by 10 afterwards adding 100 to all data points (in Python, assuming X is a NumPy array this can simply be done by `X_new = X * 10 + 100`). Now you can run DBSCAN over `X_new`.\n",
    "\n",
    "**Explain how you have to change the parameters of DBSCAN for `X_new` to get the same clusters as for `X`!**. You can ignore any corner cases related to 2.2 a).\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6eb954",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0058652",
   "metadata": {},
   "source": [
    "#### 2.2 d) Identifying Noise/Outliers with Clustering beyond DBSCAN (3 Points)\n",
    "\n",
    "Apart from DBSCAN, we also covered two other important clustering algorithms: K-Means and AGNES (Agglomerative Nesting). For all three clustering algorithms we looked in detail into their approach, and also discussed the individual strengths, weaknesses, and limitations. Particularly we saw that of these three algorithms, only DBSCAN has this explicit notion of noise points. But what about K-Means and AGNES?\n",
    "\n",
    "**Explain if K-Means and/or AGNES can potentially be utilized to identify noise/outliers in a dataset!** If your answer for an algorithm is \"No\", please provide a brief justification. If your answer for an algorithm is \"Yes\", provide a brief sketch (no pseudo code required; a basic description will do) how to use the algorithm for noise/outlier detection.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6cac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fc6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
