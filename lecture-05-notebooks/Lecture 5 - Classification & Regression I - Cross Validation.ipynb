{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 - Classification & Regression I - Cross Validation\n",
    "\n",
    "In this short notebook, we perform a \"proper\" classification task using cross validation. Most classifiers or regressors feature a set of hyperparameters (e.g., the k in KNN) that can significantly affect the results. To find the best parameter settings, we have to train and evaluate for different parameter values. \n",
    "\n",
    "However, this evaluation for of find the best parameter values cannot be done using the test set. The test set has to be unseen using he very end for the  final evaluation (once the hyperparameters have been fixed). Using the test set to tune the hyperparameters means that the test set has affected the training process. \n",
    "\n",
    "Let's get started...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the notebook\n",
    "\n",
    "Specify how plots get rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all required imports. Many of the stuff is for fancy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classification of IRIS Dataset\n",
    "\n",
    "The [IRIS dataset](https://archive.ics.uci.edu/ml/datasets/iris) is a very simple classification data for prediction the type of iris plant given 4 numerical features (all lengths in cm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.142</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>74</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.203</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>80</td>\n",
       "      <td>45</td>\n",
       "      <td>92</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.330</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>68</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>0.092</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.7</td>\n",
       "      <td>0.559</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            3      111             62              0        0  22.6   \n",
       "1            5       99             74             27        0  29.0   \n",
       "2            0       95             80             45       92  36.5   \n",
       "3            2      114             68             22        0  28.7   \n",
       "4            3       90             78              0        0  42.7   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.142   21        0  \n",
       "1                     0.203   32        0  \n",
       "2                     0.330   26        0  \n",
       "3                     0.092   25        0  \n",
       "4                     0.559   21        0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diabetes = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "# The rows are sorted, so let's shuffle them\n",
    "df_diabetes = df_diabetes.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Show the first 5 columns\n",
    "df_diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Data\n",
    "\n",
    "To allow to visualize things more easily, we consider only to input features (sepal length and sepal width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 614\n",
      "Size of test: 154\n"
     ]
    }
   ],
   "source": [
    "# Convert data to numpy arrays\n",
    "X = df_diabetes[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']].to_numpy()\n",
    "y = df_diabetes[['Outcome']].to_numpy().squeeze()\n",
    "\n",
    "# Let's go for a 80%/20% split -- you can change the value anf see its effects\n",
    "train_test_ratio = 0.80Â¶\n",
    "\n",
    "# Calculate the size of the training data (the size of the dest data is also implicitly given)\n",
    "train_set_size = int(train_test_ratio * len(X))\n",
    "\n",
    "# Split data and labels into training and test data with respect to the size of the test data\n",
    "X_train, X_test = X[:train_set_size], X[train_set_size:]\n",
    "y_train, y_test = y[:train_set_size].squeeze(), y[train_set_size:].squeeze()\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data\n",
    "\n",
    "Although we have only numerical values as input attributes there magnitudes and ranges differ noticeable. It's therefore a good idea to normalize/standardize the data. As usual, scitkit-learn makes it very convenient by providing a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) (among other methods for normalization and standardization).\n",
    "\n",
    "**Important:** We have fit the scaler on the training data `X_train` only (fitting here means to calculate the mean and standard deviation)! If we would use the `X` for that, then this would include the test data `X_test`. In this case, the test data would affect the transformation and training steps. However, `X_test` has to remain truly \"unseen\" until the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "#scaler = preprocessing.StandardScaler().fit(X)  # WRONG!!!\n",
    "\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from looking at the plot above we can see, that the \"red\" class is well separated while the \"green\" and \"blue\" classes show quite some overlap. Based on this we can expect that predicting the \"red\" class correctly will be easier than for the \"green\" and \"blue\" class.\n",
    "\n",
    "**Note:** This overlap between the \"green\" and \"blue\" class is only so pronounced because we have ignored 2 features. With respect to all 4 features, all 3 classes are quite separated and most classification models have no problem with that simple dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test KNN Classifier Using Cross-Validation\n",
    "\n",
    "### Semi-Manually K-Fold Validation\n",
    "\n",
    "We first utilize scikit-learn's  [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to split the training data into $k$ folds (here, $k=10$). The [`KFold.split()`] methods generates the folds and allows to loop over all combination of training and validation folds. Each combination contains $k-1$ training folds and 1 validation fold. For each combination we can retrain and validate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1-score was 0.714 for a k=5\n"
     ]
    }
   ],
   "source": [
    "# Initialize the best f1-score and respective k value\n",
    "k_best, f1_best = None, 0.0\n",
    "\n",
    "# Loop over a range of values for setting k\n",
    "for k in range(1, 20):\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "    f1_scores = []\n",
    "    for train_index, val_index in kf.split(X_train_transformed):\n",
    "        \n",
    "        # Create the next combination of training and validation folds\n",
    "        X_trn, X_val = X_train_transformed[train_index], X_train_transformed[val_index]\n",
    "        y_trn, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "        # Train the classifier for the current training folds\n",
    "        classifier = KNeighborsClassifier(n_neighbors=k).fit(X_trn, y_trn)\n",
    "        \n",
    "        # Predict the labels for the validation fold\n",
    "        y_pred = classifier.predict(X_val)\n",
    "\n",
    "        # Calculate the f1-score for the validation fold\n",
    "        f1_scores.append(f1_score(y_val, y_pred, average='macro'))\n",
    "        \n",
    "    # Calculate f1-score for all fold combination as the mean over all scores\n",
    "    f1_fold_mean = np.mean(f1_scores)\n",
    "    \n",
    "    # Keep track of the best f1-score and the respective k value\n",
    "    if f1_fold_mean > f1_best:\n",
    "        k_best, f1_best = k, f1_fold_mean\n",
    "        \n",
    "        \n",
    "print('The best f1-score was {:.3f} for a k={}'.format(f1_best, k_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Cross-Validation\n",
    "\n",
    "scikit-learn provides the even more convenient method [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) that does the generation of folds and spliting them into training folds and validation folds, as well as the training of a classifier for all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1-score was 0.724 for a k=5\n"
     ]
    }
   ],
   "source": [
    "# Initialize the best f1-score and respective k value\n",
    "k_best, f1_best = None, 0.0\n",
    "\n",
    "# Loop over a range of values for setting k\n",
    "for k in range(1, 20):\n",
    "    \n",
    "    # Specfify type of classifier\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # perform cross validation (here with 10 folds)\n",
    "    # f1_scores is an array containg the 10 f1-scores\n",
    "    f1_scores = cross_val_score(classifier, X_train_transformed, y_train, cv=10, scoring='f1_macro')\n",
    "    \n",
    "    # Calculate the f1-score for the current k value as the mean over all 10 f1-scores\n",
    "    f1_fold_mean = np.mean(f1_scores)\n",
    "    \n",
    "    # Keep track of the best f1-score and the respective k value\n",
    "    if f1_fold_mean > f1_best:\n",
    "        k_best, f1_best = k, f1_fold_mean\n",
    "  \n",
    "\n",
    "print('The best f1-score was {:.3f} for a k={}'.format(f1_best, k_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation on Test Data\n",
    "\n",
    "Now that we have identified the best value for $k$, we can perform the final evaluation using the test data. We can now also use the fill training data, and don't need to split it into any folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final f1-score of the KNN classifier (k=5) is: 0.680\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=k_best).fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_transformed)\n",
    "\n",
    "f1_final = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('The final f1-score of the KNN classifier (k={}) is: {:.3f}'.format(k_best, f1_final))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final score is the one to report when quantifying the quality of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "It is tempting to use the test data over and over again to find the best values for the hyperparameters for a classifier or regressor. However, this defeats the purpose of the test data which is supposed to be unseen.\n",
    "\n",
    "For finding the best parameter values, it is therefore required to split the training data further into a training and validation set. The validation set is used to evaluate a classifier for different hyperparameter values. In practice, typically several splits into training and validation data are used for each parameter setting. While different ways to generate this different splits exist, in this notebook, we used to common k-fold validation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
