{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6 - Classification & Regression II (Decision Trees)\n",
    "\n",
    "Decision Trees are a fundamental model used for classification and regression. While they typically do not yield state-of-the art performances, their inner workings lay the foundation towards more sophisticated models base on Tree Ensembles. Another important advantage of Decision Trees is that they are typically easy to interpret -- that is, by looking at the tree one can see and comprehend what pattern the model was learning.\n",
    "\n",
    "In general, Decision Trees can handle numerical and categorical. However, scikit-learn implementation \"does not support categorical variables for now.\" (see [documentation](https://scikit-learn.org/stable/modules/tree.html)). Of such detail you have to be aware off when applying off-the-shelf implementations of classification or regression algorithms on your own data. For example, a categorical feature that \"looks\" like a number such as `postal_code` will be treated as a numerical features when using the `DecisionTreeClassifier` or the `DecisionTreeRegressor` provided by scikit-learn. While the model will train without errors, the result will be off due to the misinterpretation of the data.\n",
    "\n",
    "As you will see in the examples below, `DecisionTreeClassifier` and `DecisionTreeRegressor` will only create binary decision trees, i.e., each non-leaf node will only have 2 child subtrees. \n",
    "\n",
    "\n",
    "Note that Decision Trees do not require the data to be normalized since each decision (i.e., node in the tree) is based on only a single feature. On the other hand, this also means that Decision Trees do not consider the relationship between features. We will explore the consequences in this notebook.\n",
    "\n",
    "\n",
    "Let's get started...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the notebook\n",
    "\n",
    "Specify how plots get rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all required imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.metrics import f1_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxiliary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All classification datasets in this notebook have no more the 3 labels, so 3 colors is enough\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "# Method to plot the decision boundaries (for classification)\n",
    "# Only applicable if there are 2 input features\n",
    "def plot_decision_boundaries(clf, X, y, resolution=0.01):\n",
    "\n",
    "    plt.figure()\n",
    "    margin = 0.05\n",
    "    x_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin\n",
    "    y_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution), np.arange(y_min, y_max, resolution))\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    Z = clf.predict(np.array([xx.ravel(), yy.ravel()]).T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap)\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], c=[colors[int(c)] for c in y], s=100)\n",
    "    plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)   \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Toy Data\n",
    "\n",
    "To better understand the basic characteristics of Decision Trees, we first look at the 2 small examples covered in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Relationships Between Features\n",
    "\n",
    "The first example is a small classification dataset comprising 26 data points and 2 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    (0.05, 0.65, 0), (0.65, 0.2, 0), (0.15, 0.5, 0), (0.25, 0.55, 0), (0.2, 0.4, 0), (0.3, 0.35, 0),\n",
    "    (0.4, 0.45, 0), (0.45, 0.35, 0), (0.5, 0.25, 0), (0.85, 0.05, 0), (0.6, 0.3, 0), (0.7, 0.25, 0),\n",
    "    (0.85, 0.3, 1), (0.05, 0.95, 1), (0.2, 0.9, 1), (0.35, 0.85, 1), (0.4, 0.7, 1), (0.5, 0.65, 1), \n",
    "    (0.1, 0.85, 1), (0.6, 0.5, 1), (0.7, 0.45, 1), (0.8, 0.4, 1), (0.25, 0.7, 1), (0.35, 0.85, 1), \n",
    "    (0.7, 0.6, 1), (0.8, 0.5, 1), \n",
    "])\n",
    "\n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "num_samples, num_features = X.shape\n",
    "\n",
    "print('The dataset consists of {} data points, each with {} features.'.format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c=[colors[int(c)] for c in y], s=100)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from looking at this plot, we can see that the classes could be easily separated by a diagonal line, as there is some linear relationship between the features. However, Decision Tress do not capture such relationships between features as each decision is based only on a single feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Decision Tree Classifier\n",
    "\n",
    "Since we have numerical values only, we can use the [Decision Tree implementation of scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). The implementation considers a wide range of input parameters, but we consider here only 2: `max_depth` to specify the maximum depth of the Decision Tree, and `criterion` to specify which scoring function to use to find the best split.\n",
    "\n",
    "Try changing `max_depth` and see how the resulting Decision Tree looks like. Since this is a very small dataset, it won't be very deep anyway; `max_depth=100` is just to guarantee it's maximum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=100, criterion='gini').fit(X, y)\n",
    "\n",
    "print('The Decision Tree has {} nodes.'.format(clf.tree_.node_count))\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure of the Decision Tree above gives you already useful insights. For example, as the second features `X[1]` it's used in the root node, this features is the mos \"valuable\" since it creates the best first split of complete dataset. The figures also shoes the respective thresholds, e.g., `0.575` in case of the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Decision Boundaries\n",
    "\n",
    "Again, try different values for `max_depth` and see how the decision boundaries change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that Decision Trees can only generate decision boundaries made out of vertical and horizontal sections -- in the context of this plot. Each section represent a single decision, i.e., a single node in the Decision Tree. That means, that any more intricate decision boundary has to estimated by a series of simple decision boundaries, potentially required large/deep Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting & Underfitting\n",
    "\n",
    "We now perform the same steps as above for a different toy dataset to illustrate the notion of overfitting and underfitting in the context of Decision Trees. This dataset again reflects the example used in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    (0.05, 0.4, 0), (0.15, 0.1, 0), (0.15, 0.35, 0), (0.2, 0.25, 0), (0.4, 0.4, 0), (0.45, 0.3, 0), \n",
    "    (0.95, 0.4, 1), (0.8, 0.4, 1), (0.65, 0.05, 0), (0.7, 0.15, 0), (0.85, 0.1, 0), (0.8, 0.3, 1),\n",
    "    (0.6, 0.42, 0), (0.4, 0.1, 0), (0.63, 0.32, 0),\n",
    "    (0.1, 0.55, 1), (0.08, 0.7, 1), (0.32, 0.55, 1), (0.53, 0.75, 1), (0.25, 0.78, 1), (0.9, 0.9, 1),\n",
    "    (0.38, 0.85, 1), (0.65, 0.9, 1), (0.95, 0.6, 1), (0.80, 0.55, 1), (0.55, 0.6, 1), (0.05, 0.85, 1),\n",
    "    (0.85, 0.7, 1), (0.32, 0.89, 1), (0.95, 0.05, 0), (0.95, 0.15, 0), (0.92, 0.3, 1)\n",
    "])\n",
    "\n",
    "# Add \"outlier\" point\n",
    "data = np.concatenate((data, np.array([(0.32, 0.7, 0)])))\n",
    "\n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "num_samples, num_features = X.shape\n",
    "\n",
    "print('The dataset consists of {} data points, each with {} features.'.format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c=[colors[int(c)] for c in y], s=100)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Decision Tree Classifier\n",
    "\n",
    "As a above, play with the value of `max_depth` and see how the resulting Decision Tree looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=3, criterion='gini')\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('The Decision Tree has {} nodes.'.format(clf.tree_.node_count))\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Decision Boundaries\n",
    "\n",
    "Using different values for `max_depth` will again change the decision boundaries. See how different values effect the area around the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification of IRIS Dataset\n",
    "\n",
    "The [IRIS dataset](https://archive.ics.uci.edu/ml/datasets/iris) is a very simple classification data for prediction the type of iris plant given 4 numerical features (all lengths in cm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "# Convert the species name to numerical categories 0, 1, 2\n",
    "df['species'] = pd.factorize(df['species'])[0]\n",
    "\n",
    "# Show the first 5 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: 2 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training and Test Data\n",
    "\n",
    "To allow to visualize things more easily, we first consider only two input features (sepal length and sepal width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy arrays\n",
    "X = df[['sepal_length', 'sepal_width']].to_numpy()\n",
    "y = df[['species']].to_numpy().squeeze()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Decision Tree Classifier\n",
    "\n",
    "**Important:** Decision Trees (at least the scikit-learn implementations) have random element, since different features and their respective thresholds might result in the same information gain. That means, two or more splits are equally good, and it depends on the order in which features are processed which feature will \"win\".\n",
    "\n",
    "The order of features is randomly permutated in case of scikit-learns `DecisionTreeClassifier` and `DecisionTreeRegressor`. So without setting the `random_state` two runs on the same data might yield different Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=5, criterion='gini', random_state=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('The Decision Tree has {} nodes.'.format(clf.tree_.node_count))\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering only the two features `sepal_length` and `sepal_width` we can see that the red and green labels are quite overlapping, i.e., not really separable. And classification model will have problems with that, but particularly challenging for Decision Trees since can only construct arbitrary decision boundaries based on simple boundary sections.\n",
    "\n",
    "Change `max_depth` and see how the size and shape of the decision boundaries change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Best Value for `max_depth`\n",
    "\n",
    "We already know that the size/depth of a Decision Tree -- here specified by the hyperparameter `max_depth` -- relates to the notion of overfitting and underfitting. The the following we find the best value for `max_depth` by trying a series of choice the record the resulting f1 score for each classifiers.\n",
    "\n",
    "**Important:** We are a bit sloppy here since we use the test set for this. As you already know, hyperparameter tuning should be done using a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 20\n",
    "\n",
    "# Keep track of depth and f1 scores for plotting\n",
    "ds, f1s = [], []\n",
    "\n",
    "# Loop over all values for max_depth\n",
    "for d in range(1, max_depth+1):\n",
    "    ds.append(d)\n",
    "    # Train Decision Tree classifier for current value of max_depth\n",
    "    clf = DecisionTreeClassifier(max_depth=d, criterion='gini', random_state=10).fit(X_train, y_train)\n",
    "    # Predict class labels for test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calculate f1 score between predictions and ground truth\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f1s.append(f1)\n",
    "    \n",
    "print('A maximum depth of {} yields the best f1 score of {:.3f}'.format(ds[np.argmax(f1s)], np.max(f1s), ))        \n",
    "    \n",
    "# Plot the results (max_depth vs. f1.score)\n",
    "plt.figure()\n",
    "plt.plot(ds, f1s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example (and with this `random_state`), a `max_depth` of 6 or 7 yields the best results. You can go back and plot the decision boundaries for these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: 4 Features\n",
    "\n",
    "Let's now consider all 4 features of the IRIS dataset. Nothing of substance changes, we only can no longer plot the decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy arrays\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].to_numpy()\n",
    "y = df[['species']].to_numpy().squeeze()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=5, criterion='gini', random_state=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('The Decision Tree has {} nodes.'.format(clf.tree_.node_count))\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Best Value for `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 20\n",
    "\n",
    "# Keep track of depth and f1 scores for plotting\n",
    "ds, f1s = [], []\n",
    "\n",
    "# Loop over all values for max_depth\n",
    "for d in range(1, max_depth+1):\n",
    "    ds.append(d)\n",
    "    # Train Decision Tree classifier for current value of max_depth\n",
    "    clf = DecisionTreeClassifier(max_depth=d, criterion='gini', random_state=10).fit(X_train, y_train)\n",
    "    # Predict class labels for test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calculate f1 score between predictions and ground truth\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f1s.append(f1)\n",
    "    \n",
    "print('A maximum depth of {} yields the best f1 score of {:.3f}'.format(ds[np.argmax(f1s)], np.max(f1s), ))\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(ds, f1s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the results look much better, perfect in fact, although this will slightly differ for different splits in to training an test data. The reason for this is because the IRIS dataset is rather simple and the data points can be separated quite easily when considering all 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression of Hitters Dataset\n",
    "\n",
    "Lastly, let's look at an example using Decision Trees for regression.\n",
    "\n",
    "This [Hitters dataset](https://www.kaggle.com/floser/hitters) was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a couple of categorical features. But for this example, it's not a problem since all of them a binary so mapping them to 0 and 1 is a solution. Note that there is no difference whether 0/1 are treated as a categorical or numerical feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hitters.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "df['League'] = pd.factorize(df['League'])[0]\n",
    "df['NewLeague'] = pd.factorize(df['NewLeague'])[0]\n",
    "df['Division'] = pd.factorize(df['Division'])[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years', 'CAtBat', 'CHits', 'CHmRun', 'CRuns', 'CRBI', \n",
    "        'CWalks', 'League', 'Division', 'PutOuts', 'Assists', 'Errors']].to_numpy()\n",
    "\n",
    "y = df[['Salary']].to_numpy().squeeze()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Decision Tree Regressor\n",
    "\n",
    "Training a regressor is basically the same as training a classifier. We have seen in the lecture that A Decision Tree for regression and in for classification are very similar; the core differences is only in the calculation of the impurity since we now have real values instead of labels as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DecisionTreeRegressor(max_depth=5, random_state=10).fit(X_train, y_train)\n",
    "\n",
    "print('The Decision Tree has {} nodes.'.format(reg.tree_.node_count))\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Value for `max_depth`\n",
    "\n",
    "We can use almost the same code as above to find the best value of `max_depth`. We only have to change the evaluation metric from f1 to RSME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_depth = 20\n",
    "\n",
    "# Keep track of depth and RSMEs for plotting\n",
    "ds, rsmes = [], []\n",
    "\n",
    "for d in range(1, max_depth+1):\n",
    "    ds.append(d)\n",
    "    # Train Decision Tree regressor for current value of max_depth\n",
    "    reg = DecisionTreeRegressor(max_depth=d, random_state=10).fit(X_train, y_train)\n",
    "    # Predict output values for test set\n",
    "    y_pred = reg.predict(X_test)\n",
    "    # Calculate RSME between predictions and ground truth\n",
    "    rsme = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    rsmes.append(rsme)\n",
    "    \n",
    "    \n",
    "print('A maximum depth of {} yields the best RSME of {:.3f}'.format(ds[np.argmin(rsmes)], np.min(rsmes), ))    \n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(ds, rsmes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above the  best value for `max_depth` is 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduced and experimented with Decision Trees. The big advantage of Decision Trees is that one can directly inspect and interpret the resulting tree -- this becomes much harder to impossible for tree ensembles. So even you plan to use tree ensembles, as they typically perform better, it is always to good idea to first train single Decision Trees (with different parameter). It gives a first idea about the results but can also help finding good initial values for hyperparameter tuning for the more complex tree ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
